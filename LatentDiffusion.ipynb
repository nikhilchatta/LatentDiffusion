{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae68fd8-f80b-4f65-87f5-5c0bc2cfb7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.20.1)\n",
      "Requirement already satisfied: pytorch-lightning in ./.local/lib/python3.9/site-packages (1.7.7)\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.9/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-image in ./.local/lib/python3.9/site-packages (0.19.3)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.9/site-packages (3.8.4)\n",
      "Requirement already satisfied: lpips in ./.local/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: torchmetrics in ./.local/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib64/python3.9/site-packages (from torchvision) (10.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1minvalid-installed-package\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot process installed package pytorch-lightning 1.7.7 in '/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages' because it has an invalid requirement:\n",
      "\u001b[31m│\u001b[0m .* suffix can only be used with `==` or `!=` operators\n",
      "\u001b[31m│\u001b[0m     torch (>=1.9.*)\n",
      "\u001b[31m│\u001b[0m            ~~~~~~^\n",
      "\u001b[31m╰─>\u001b[0m Starting with pip \u001b[1;36m24.1\u001b[0m, packages with invalid requirements can not be processed.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: To proceed this package must be uninstalled.\n",
      " All libraries installed and imported.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision pytorch-lightning opencv-python scikit-image matplotlib lpips torchmetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\" All libraries installed and imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c0d651-6a73-44b4-96a2-8fca29917ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple VAE for CIFAR-10\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # (B, 32, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # (B, 64, 8, 8)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # (B, 128, 4, 4)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128*4*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(128*4*4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 128*4*4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # (B, 64, 8, 8)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),   # (B, 32, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),    # (B, 3, 32, 32)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "    \n",
    "    def loss_function(self, recons, input, mu, log_var):\n",
    "        recons_loss = F.mse_loss(recons, input, reduction='mean')\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / input.size(0)\n",
    "        return recons_loss + 0.0005 * kld_loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        recons, mu, log_var = self.forward(x)\n",
    "        loss = self.loss_function(recons, x, mu, log_var)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5647c862-fa3d-41dc-9e77-68968d6d9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 63.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/tarfile.py:2268: RuntimeWarning: The default behavior of tarfile extraction has been changed to disallow common exploits (including CVE-2007-4559). By default, absolute/parent paths are disallowed and some mode bits are cleared. See https://access.redhat.com/articles/7004769 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CIFAR-10 dataset loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Converts to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# CIFAR-10 Dataset\n",
    "train_dataset = CIFAR10(root=\"./\", train=True, download=True, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\" CIFAR-10 dataset loaded and ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260fa47c-7b10-4bcb-b011-25abfea8f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /users/PFS0270/nikhilchatta/lightning_logs/version_660196/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | encoder       | Sequential | 165 K \n",
      "1 | fc_mu         | Linear     | 524 K \n",
      "2 | fc_var        | Linear     | 524 K \n",
      "3 | decoder_input | Linear     | 526 K \n",
      "4 | decoder       | Sequential | 165 K \n",
      "---------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.626     Total estimated model params size (MB)\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c84194cb0f4154bd16e5b986caa7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VAE training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "vae_model = VAE(latent_dim=256)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\", devices=\"1\", precision=32)\n",
    "\n",
    "# Train\n",
    "trainer.fit(vae_model, train_loader)\n",
    "\n",
    "# Save trained VAE weights\n",
    "torch.save(vae_model.state_dict(), \"vae_cifar10.pth\")\n",
    "\n",
    "print(\" VAE training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47862053-3d71-4a1f-9a88-27148d83dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      " UNet built correctly for Latent Diffusion!\n"
     ]
    }
   ],
   "source": [
    "# Correct UNet for latent diffusion\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=256, time_emb_dim=256):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Timestep embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(latent_dim, 512, 1)\n",
    "        self.conv2 = nn.Conv1d(512, 512, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.Conv1d(512, 512, 1)\n",
    "        self.deconv2 = nn.Conv1d(512, latent_dim, 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # x: (batch, latent_dim)\n",
    "        # t: (batch,)\n",
    "        \n",
    "        t = t.unsqueeze(-1).float()  # (batch, 1)\n",
    "        t_emb = self.time_mlp(t)     # (batch, time_emb_dim)\n",
    "        t_emb = t_emb.unsqueeze(-1)  # (batch, time_emb_dim, 1)\n",
    "        \n",
    "        x = x.unsqueeze(-1)          # (batch, latent_dim, 1)\n",
    "        \n",
    "        # Add timestep embedding\n",
    "        x = x + t_emb\n",
    "        \n",
    "        # UNet Forward\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        \n",
    "        return x.squeeze(-1)  # (batch, latent_dim)\n",
    "\n",
    "# Initialize UNet\n",
    "unet_model = UNet(latent_dim=256)\n",
    "\n",
    "# Trainer: Use only 1 GPU without setting any strategy\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\", devices=1, precision=32)\n",
    "\n",
    "# Dummy input to check\n",
    "x = torch.randn(4, 256)\n",
    "t = torch.randint(0, 1000, (4,))\n",
    "out = unet_model(x, t)\n",
    "print(out.shape)  # Should be (4, 256)\n",
    "\n",
    "print(\" UNet built correctly for Latent Diffusion!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce0bf44d-26bb-4a80-b50f-ade85f5b4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Diffusion Class\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, timesteps)  # CPU tensor\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        #  move alpha_hat to same device as x_start\n",
    "        device = x_start.device\n",
    "        alpha_hat = self.alpha_hat.to(device)\n",
    "\n",
    "        sqrt_alpha_hat = torch.sqrt(alpha_hat[t]).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t]).unsqueeze(1)\n",
    "        \n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"\n",
    "        Estimate x_0 from x_t and predicted noise\n",
    "        \"\"\"\n",
    "        device = x_t.device\n",
    "        alpha_hat = self.alpha_hat.to(device)\n",
    "\n",
    "        sqrt_alpha_hat = torch.sqrt(alpha_hat[t]).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t]).unsqueeze(1)\n",
    "        \n",
    "        return (x_t - sqrt_one_minus_alpha_hat * noise) / sqrt_alpha_hat\n",
    "\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse sampling step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        noise_pred = model(x_t, t)\n",
    "        x_start = self.predict_start_from_noise(x_t, t, noise_pred)\n",
    "    \n",
    "        device = x_t.device\n",
    "        beta = self.beta.to(device)\n",
    "        alpha = self.alpha.to(device)\n",
    "        alpha_hat = self.alpha_hat.to(device)\n",
    "    \n",
    "        noise = torch.randn_like(x_t)\n",
    "        nonzero_mask = (t != 0).float().unsqueeze(1)  # (batch_size, 1)\n",
    "    \n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t]).unsqueeze(1)\n",
    "        out = torch.sqrt(alpha[t]).unsqueeze(1) * x_start + sqrt_one_minus_alpha_hat * noise\n",
    "\n",
    "        return out * nonzero_mask + x_start * (1 - nonzero_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0525ebb7-5e8c-4991-8efa-316837234223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDM Trainer (UNet + Diffusion)\n",
    "\n",
    "class LDM(pl.LightningModule):\n",
    "    def __init__(self, vae, unet, diffusion, latent_dim=256, timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.unet = unet\n",
    "        self.diffusion = diffusion\n",
    "        self.latent_dim = latent_dim\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Freeze VAE (only use encoder)\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        \n",
    "        # 1. Encode images to latent space\n",
    "        mu, log_var = self.vae.encode(imgs)\n",
    "        z = self.vae.reparameterize(mu, log_var)  # latent vector (batch, latent_dim)\n",
    "        \n",
    "        # 2. Sample random timesteps\n",
    "        batch_size = z.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=self.device).long()\n",
    "        \n",
    "        # 3. Add noise\n",
    "        noise = torch.randn_like(z)\n",
    "        z_noisy = self.diffusion.q_sample(z, t, noise=noise)\n",
    "        \n",
    "        # 4. Predict noise with UNet\n",
    "        noise_pred = self.unet(z_noisy, t)\n",
    "        \n",
    "        # 5. Loss: MSE between true noise and predicted noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.unet.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84fad98c-1fbb-4619-a1de-44755a012a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.660196/ipykernel_3271377/30379419.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_model.load_state_dict(torch.load(\"vae_cifar10.pth\", map_location=\"cpu\"))\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | vae  | VAE  | 1.9 M \n",
      "1 | unet | UNet | 854 K \n",
      "------------------------------\n",
      "854 K     Trainable params\n",
      "1.9 M     Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.044    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d25c936003949249e4ccdf2edb08404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LDM training finished and UNet model saved!\n"
     ]
    }
   ],
   "source": [
    "# Load trained VAE\n",
    "vae_model = VAE(latent_dim=256)\n",
    "vae_model.load_state_dict(torch.load(\"vae_cifar10.pth\", map_location=\"cpu\"))\n",
    "vae_model.eval()  # important\n",
    "\n",
    "# Initialize UNet\n",
    "unet_model = UNet(latent_dim=256)\n",
    "\n",
    "# Initialize Diffusion\n",
    "diffusion = Diffusion(timesteps=1000)\n",
    "\n",
    "# Initialize LDM\n",
    "ldm_model = LDM(vae=vae_model, unet=unet_model, diffusion=diffusion, latent_dim=256, timesteps=1000)\n",
    "\n",
    "# Trainer (use single GPU, no distributed)\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\", devices=1, precision=32)\n",
    "\n",
    "# Train LDM\n",
    "trainer.fit(ldm_model, train_loader)\n",
    "\n",
    "# Save UNet after training\n",
    "torch.save(ldm_model.unet.state_dict(), \"unet_ldm_cifar10.pth\")\n",
    "\n",
    "print(\" LDM training finished and UNet model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57423941-994e-498e-9047-bb865023a3b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.660196/ipykernel_3271377/2463865566.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet_model.load_state_dict(torch.load(\"unet_ldm_cifar10.pth\", map_location=\"cuda\"))\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/matplotlib/cm.py:494: RuntimeWarning: invalid value encountered in cast\n",
      "  xx = (xx * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sampling function\n",
    "def sample_ldm(unet, vae, diffusion, num_samples=8, latent_dim=256, timesteps=1000):\n",
    "    unet.eval()\n",
    "    vae.eval()\n",
    "    \n",
    "    device = next(unet.parameters()).device\n",
    "    \n",
    "    # Start from pure Gaussian noise\n",
    "    x = torch.randn(num_samples, latent_dim).to(device)\n",
    "    \n",
    "    # Reverse diffusion (iteratively remove noise)\n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "        x = diffusion.p_sample(unet, x, t_batch)\n",
    "    \n",
    "    # Decode latent vectors into images\n",
    "    with torch.no_grad():\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "    \n",
    "    return decoded_imgs\n",
    "\n",
    "# Load the trained UNet\n",
    "unet_model = UNet(latent_dim=256)\n",
    "unet_model.load_state_dict(torch.load(\"unet_ldm_cifar10.pth\", map_location=\"cuda\"))\n",
    "unet_model = unet_model.cuda()\n",
    "\n",
    "# Sampling\n",
    "generated_images = sample_ldm(unet=unet_model, vae=vae_model.cuda(), diffusion=Diffusion(timesteps=1000), num_samples=8)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 8, figsize=(20, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = (generated_images[i].permute(1, 2, 0) + 1) / 2  # de-normalize from [-1,1] to [0,1]\n",
    "    img = img.clamp(0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e1df533-5bef-4141-9861-ab9d8b50af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated 1000 images.\n"
     ]
    }
   ],
   "source": [
    "# Generate 1000 samples\n",
    "def generate_samples(unet, vae, diffusion, num_samples=1000, latent_dim=256, timesteps=1000):\n",
    "    unet.eval()\n",
    "    vae.eval()\n",
    "    \n",
    "    device = next(unet.parameters()).device\n",
    "    samples = []\n",
    "    \n",
    "    batch_size = 100  # Generate 100 at a time (10 batches)\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "        samples.append(decoded_imgs)\n",
    "    \n",
    "    samples = torch.cat(samples, dim=0)  # (1000, 3, 32, 32)\n",
    "    return samples\n",
    "\n",
    "# Generate 1000 fake images\n",
    "generated_images = generate_samples(\n",
    "    unet=unet_model,\n",
    "    vae=vae_model.cuda(),\n",
    "    diffusion=Diffusion(timesteps=1000),\n",
    "    num_samples=1000\n",
    ")\n",
    "\n",
    "print(f\" Generated {generated_images.shape[0]} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05301370-d47d-424b-a04a-7830d3a9bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      " Running Round 1/20...\n",
      "Round 1 PSNR: 8.3465 dB\n",
      " Running Round 2/20...\n",
      "Round 2 PSNR: 8.3080 dB\n",
      " Running Round 3/20...\n",
      "Round 3 PSNR: 8.3031 dB\n",
      " Running Round 4/20...\n",
      "Round 4 PSNR: 8.3171 dB\n",
      " Running Round 5/20...\n",
      "Round 5 PSNR: 8.3566 dB\n",
      " Running Round 6/20...\n",
      "Round 6 PSNR: 8.3046 dB\n",
      " Running Round 7/20...\n",
      "Round 7 PSNR: 8.2641 dB\n",
      " Running Round 8/20...\n",
      "Round 8 PSNR: 8.3315 dB\n",
      " Running Round 9/20...\n",
      "Round 9 PSNR: 8.3743 dB\n",
      " Running Round 10/20...\n",
      "Round 10 PSNR: 8.2931 dB\n",
      " Running Round 11/20...\n",
      "Round 11 PSNR: 8.2423 dB\n",
      " Running Round 12/20...\n",
      "Round 12 PSNR: 8.2368 dB\n",
      " Running Round 13/20...\n",
      "Round 13 PSNR: 8.3908 dB\n",
      " Running Round 14/20...\n",
      "Round 14 PSNR: 8.3702 dB\n",
      " Running Round 15/20...\n",
      "Round 15 PSNR: 8.3725 dB\n",
      " Running Round 16/20...\n",
      "Round 16 PSNR: 8.3377 dB\n",
      " Running Round 17/20...\n",
      "Round 17 PSNR: 8.3331 dB\n",
      " Running Round 18/20...\n",
      "Round 18 PSNR: 8.3323 dB\n",
      " Running Round 19/20...\n",
      "Round 19 PSNR: 8.3521 dB\n",
      " Running Round 20/20...\n",
      "Round 20 PSNR: 8.3227 dB\n",
      "\n",
      " Final Average PSNR over 20 rounds: 8.3245 dB ± 0.1848\n"
     ]
    }
   ],
   "source": [
    "# Imports (if needed)\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_metric\n",
    "\n",
    "# De-normalization helper\n",
    "def denormalize(imgs):\n",
    "    return (imgs + 1) / 2\n",
    "\n",
    "# Create Diffusion object correctly with 1000 timesteps\n",
    "diffusion_obj = Diffusion(timesteps=1000)\n",
    "\n",
    "# CIFAR-10 real images (download/test set)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "cifar_test = CIFAR10(root=\"./\", train=False, download=True, transform=transform)\n",
    "real_images = torch.stack([cifar_test[i][0] for i in range(1000)])  # (1000, 3, 32, 32)\n",
    "\n",
    "# Function to generate 1000 samples and compute PSNR\n",
    "def compute_psnr_round(unet, vae, diffusion, real_images, num_samples=1000, latent_dim=256, timesteps=1000):\n",
    "    device = next(unet.parameters()).device\n",
    "    samples = []\n",
    "    \n",
    "    batch_size = 100\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "        samples.append(decoded_imgs)\n",
    "    \n",
    "    generated_images = torch.cat(samples, dim=0)\n",
    "    generated_images_denorm = denormalize(generated_images)\n",
    "    real_images_denorm = denormalize(real_images)\n",
    "    \n",
    "    psnr_scores = []\n",
    "    for i in range(num_samples):\n",
    "        fake = generated_images_denorm[i].permute(1, 2, 0).detach().numpy()\n",
    "        real = real_images_denorm[i].permute(1, 2, 0).detach().numpy()\n",
    "        score = psnr_metric(real, fake, data_range=1.0)\n",
    "        psnr_scores.append(score)\n",
    "    \n",
    "    return sum(psnr_scores) / len(psnr_scores)\n",
    "\n",
    "# Run PSNR for 20 rounds\n",
    "# Run PSNR for 20 rounds with 50 timesteps\n",
    "psnr_all_rounds = []\n",
    "\n",
    "for round_num in range(20):\n",
    "    print(f\" Running Round {round_num+1}/20...\")\n",
    "    psnr_score = compute_psnr_round(\n",
    "        unet=unet_model,\n",
    "        vae=vae_model.cuda(),\n",
    "        diffusion=diffusion_obj,\n",
    "        real_images=real_images,\n",
    "        timesteps=50  # 50 steps only\n",
    "    )\n",
    "    psnr_all_rounds.append(psnr_score)\n",
    "    print(f\"Round {round_num+1} PSNR: {psnr_score:.4f} dB\")\n",
    "\n",
    "# Final Report\n",
    "psnr_mean = sum(psnr_all_rounds) / len(psnr_all_rounds)\n",
    "psnr_std = (sum((x - psnr_mean)**2 for x in psnr_all_rounds))**0.5\n",
    "\n",
    "print(f\"\\n Final Average PSNR over 20 rounds: {psnr_mean:.4f} dB ± {psnr_std:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "979ee020-1d5f-4bd0-b715-acbedd057b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running SSIM Round 1/20...\n",
      "Round 1 SSIM: 0.0275\n",
      " Running SSIM Round 2/20...\n",
      "Round 2 SSIM: 0.0336\n",
      " Running SSIM Round 3/20...\n",
      "Round 3 SSIM: 0.0312\n",
      " Running SSIM Round 4/20...\n",
      "Round 4 SSIM: 0.0310\n",
      " Running SSIM Round 5/20...\n",
      "Round 5 SSIM: 0.0304\n",
      " Running SSIM Round 6/20...\n",
      "Round 6 SSIM: 0.0269\n",
      " Running SSIM Round 7/20...\n",
      "Round 7 SSIM: 0.0269\n",
      " Running SSIM Round 8/20...\n",
      "Round 8 SSIM: 0.0302\n",
      " Running SSIM Round 9/20...\n",
      "Round 9 SSIM: 0.0274\n",
      " Running SSIM Round 10/20...\n",
      "Round 10 SSIM: 0.0331\n",
      " Running SSIM Round 11/20...\n",
      "Round 11 SSIM: 0.0319\n",
      " Running SSIM Round 12/20...\n",
      "Round 12 SSIM: 0.0302\n",
      " Running SSIM Round 13/20...\n",
      "Round 13 SSIM: 0.0337\n",
      " Running SSIM Round 14/20...\n",
      "Round 14 SSIM: 0.0297\n",
      " Running SSIM Round 15/20...\n",
      "Round 15 SSIM: 0.0296\n",
      " Running SSIM Round 16/20...\n",
      "Round 16 SSIM: 0.0304\n",
      " Running SSIM Round 17/20...\n",
      "Round 17 SSIM: 0.0339\n",
      " Running SSIM Round 18/20...\n",
      "Round 18 SSIM: 0.0362\n",
      " Running SSIM Round 19/20...\n",
      "Round 19 SSIM: 0.0297\n",
      " Running SSIM Round 20/20...\n",
      "Round 20 SSIM: 0.0307\n",
      "\n",
      " Final Average SSIM over 20 rounds: 0.0307 ± 0.0024\n"
     ]
    }
   ],
   "source": [
    "# Imports if needed\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "\n",
    "# De-normalization helper (if not already defined)\n",
    "def denormalize(imgs):\n",
    "    return (imgs + 1) / 2\n",
    "\n",
    "# Function to generate 1000 samples and compute SSIM\n",
    "def compute_ssim_round(unet, vae, diffusion, real_images, num_samples=1000, latent_dim=256, timesteps=50):\n",
    "    device = next(unet.parameters()).device\n",
    "    samples = []\n",
    "    \n",
    "    batch_size = 100\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "        samples.append(decoded_imgs)\n",
    "    \n",
    "    generated_images = torch.cat(samples, dim=0)\n",
    "    generated_images_denorm = denormalize(generated_images)\n",
    "    real_images_denorm = denormalize(real_images)\n",
    "    \n",
    "    ssim_scores = []\n",
    "    for i in range(num_samples):\n",
    "        fake = generated_images_denorm[i].permute(1, 2, 0).detach().numpy()\n",
    "        real = real_images_denorm[i].permute(1, 2, 0).detach().numpy()\n",
    "        score = ssim_metric(real, fake, data_range=1.0, channel_axis=2)\n",
    "        ssim_scores.append(score)\n",
    "    \n",
    "    return sum(ssim_scores) / len(ssim_scores)\n",
    "\n",
    "# Now run SSIM for 20 rounds\n",
    "ssim_all_rounds = []\n",
    "\n",
    "for round_num in range(20):\n",
    "    print(f\" Running SSIM Round {round_num+1}/20...\")\n",
    "    ssim_score = compute_ssim_round(\n",
    "        unet=unet_model,\n",
    "        vae=vae_model.cuda(),\n",
    "        diffusion=diffusion_obj,  # using 50 step diffusion\n",
    "        real_images=real_images,\n",
    "        timesteps=50\n",
    "    )\n",
    "    ssim_all_rounds.append(ssim_score)\n",
    "    print(f\"Round {round_num+1} SSIM: {ssim_score:.4f}\")\n",
    "\n",
    "# Final Report\n",
    "ssim_mean = sum(ssim_all_rounds) / len(ssim_all_rounds)\n",
    "ssim_std = (sum([(x - ssim_mean)**2 for x in ssim_all_rounds]) / len(ssim_all_rounds))**0.5\n",
    "\n",
    "print(f\"\\n Final Average SSIM over 20 rounds: {ssim_mean:.4f} ± {ssim_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c556f90-6e79-4353-9596-871d38201376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Perceptual Round 1/20...\n",
      "Round 1 Perceptual Similarity: 0.2092\n",
      " Running Perceptual Round 2/20...\n",
      "Round 2 Perceptual Similarity: 0.2119\n",
      " Running Perceptual Round 3/20...\n",
      "Round 3 Perceptual Similarity: 0.2109\n",
      " Running Perceptual Round 4/20...\n",
      "Round 4 Perceptual Similarity: 0.2105\n",
      " Running Perceptual Round 5/20...\n",
      "Round 5 Perceptual Similarity: 0.2104\n",
      " Running Perceptual Round 6/20...\n",
      "Round 6 Perceptual Similarity: 0.2109\n",
      " Running Perceptual Round 7/20...\n",
      "Round 7 Perceptual Similarity: 0.2096\n",
      " Running Perceptual Round 8/20...\n",
      "Round 8 Perceptual Similarity: 0.2112\n",
      " Running Perceptual Round 9/20...\n",
      "Round 9 Perceptual Similarity: 0.2106\n",
      " Running Perceptual Round 10/20...\n",
      "Round 10 Perceptual Similarity: 0.2109\n",
      " Running Perceptual Round 11/20...\n",
      "Round 11 Perceptual Similarity: 0.2110\n",
      " Running Perceptual Round 12/20...\n",
      "Round 12 Perceptual Similarity: 0.2110\n",
      " Running Perceptual Round 13/20...\n",
      "Round 13 Perceptual Similarity: 0.2103\n",
      " Running Perceptual Round 14/20...\n",
      "Round 14 Perceptual Similarity: 0.2111\n",
      " Running Perceptual Round 15/20...\n",
      "Round 15 Perceptual Similarity: 0.2105\n",
      " Running Perceptual Round 16/20...\n",
      "Round 16 Perceptual Similarity: 0.2113\n",
      " Running Perceptual Round 17/20...\n",
      "Round 17 Perceptual Similarity: 0.2107\n",
      " Running Perceptual Round 18/20...\n",
      "Round 18 Perceptual Similarity: 0.2108\n",
      " Running Perceptual Round 19/20...\n",
      "Round 19 Perceptual Similarity: 0.2115\n",
      " Running Perceptual Round 20/20...\n",
      "Round 20 Perceptual Similarity: 0.2111\n",
      "\n",
      " Final Average Perceptual Similarity over 20 rounds: 0.2108 ± 0.0006\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load Pretrained VGG16 model (for feature extraction)\n",
    "vgg16 = models.vgg16(pretrained=True).features.eval().cuda()\n",
    "\n",
    "# We will use features up to layer 16 (before classifier starts)\n",
    "vgg_layers = torch.nn.Sequential(*list(vgg16.children())[:16])\n",
    "\n",
    "# Freeze VGG parameters\n",
    "for param in vgg_layers.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Normalize images as required by VGG\n",
    "vgg_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Denormalization helper (already defined earlier if needed)\n",
    "def denormalize(imgs):\n",
    "    return (imgs + 1) / 2\n",
    "\n",
    "# Function to compute perceptual similarity between batches\n",
    "def compute_perceptual_round(unet, vae, diffusion, real_images, num_samples=1000, latent_dim=256, timesteps=50):\n",
    "    device = next(unet.parameters()).device\n",
    "    samples = []\n",
    "    \n",
    "    batch_size = 100\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "        samples.append(decoded_imgs)\n",
    "    \n",
    "    generated_images = torch.cat(samples, dim=0)\n",
    "    generated_images_denorm = denormalize(generated_images)\n",
    "    real_images_denorm = denormalize(real_images)\n",
    "    \n",
    "    perceptual_scores = []\n",
    "    for i in range(num_samples):\n",
    "        fake = generated_images_denorm[i].unsqueeze(0).cuda()  # (1,3,32,32)\n",
    "        real = real_images_denorm[i].unsqueeze(0).cuda()\n",
    "        \n",
    "        # Resize to 224x224 for VGG\n",
    "        fake_resized = F.interpolate(fake, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        real_resized = F.interpolate(real, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize for VGG\n",
    "        fake_norm = vgg_normalize(fake_resized.squeeze(0)).unsqueeze(0)\n",
    "        real_norm = vgg_normalize(real_resized.squeeze(0)).unsqueeze(0)\n",
    "        \n",
    "        # Extract features\n",
    "        fake_feat = vgg_layers(fake_norm)\n",
    "        real_feat = vgg_layers(real_norm)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        similarity = F.cosine_similarity(fake_feat.flatten(1), real_feat.flatten(1)).mean().item()\n",
    "        \n",
    "        perceptual_scores.append(similarity)\n",
    "    \n",
    "    return sum(perceptual_scores) / len(perceptual_scores)\n",
    "\n",
    "# Run Perceptual Similarity for 20 rounds\n",
    "perceptual_all_rounds = []\n",
    "\n",
    "for round_num in range(20):\n",
    "    print(f\" Running Perceptual Round {round_num+1}/20...\")\n",
    "    perceptual_score = compute_perceptual_round(\n",
    "        unet=unet_model,\n",
    "        vae=vae_model.cuda(),\n",
    "        diffusion=diffusion_obj,  # using 50 steps diffusion\n",
    "        real_images=real_images,\n",
    "        timesteps=50\n",
    "    )\n",
    "    perceptual_all_rounds.append(perceptual_score)\n",
    "    print(f\"Round {round_num+1} Perceptual Similarity: {perceptual_score:.4f}\")\n",
    "\n",
    "# Final Report\n",
    "perceptual_mean = sum(perceptual_all_rounds) / len(perceptual_all_rounds)\n",
    "perceptual_std = (sum([(x - perceptual_mean)**2 for x in perceptual_all_rounds]) / len(perceptual_all_rounds))**0.5\n",
    "\n",
    "print(f\"\\n Final Average Perceptual Similarity over 20 rounds: {perceptual_mean:.4f} ± {perceptual_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afb43cde-34ae-42e1-940a-a43d48a41931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running FID Round 1/20...\n",
      "Round 1 FID: 146.7681\n",
      " Running FID Round 2/20...\n",
      "Round 2 FID: 148.8888\n",
      " Running FID Round 3/20...\n",
      "Round 3 FID: 148.5847\n",
      " Running FID Round 4/20...\n",
      "Round 4 FID: 148.5047\n",
      " Running FID Round 5/20...\n",
      "Round 5 FID: 147.8991\n",
      " Running FID Round 6/20...\n",
      "Round 6 FID: 147.3239\n",
      " Running FID Round 7/20...\n",
      "Round 7 FID: 146.5880\n",
      " Running FID Round 8/20...\n",
      "Round 8 FID: 148.4425\n",
      " Running FID Round 9/20...\n",
      "Round 9 FID: 148.1095\n",
      " Running FID Round 10/20...\n",
      "Round 10 FID: 148.9086\n",
      " Running FID Round 11/20...\n",
      "Round 11 FID: 147.7468\n",
      " Running FID Round 12/20...\n",
      "Round 12 FID: 146.8986\n",
      " Running FID Round 13/20...\n",
      "Round 13 FID: 150.1813\n",
      " Running FID Round 14/20...\n",
      "Round 14 FID: 146.9177\n",
      " Running FID Round 15/20...\n",
      "Round 15 FID: 147.4944\n",
      " Running FID Round 16/20...\n",
      "Round 16 FID: 148.4953\n",
      " Running FID Round 17/20...\n",
      "Round 17 FID: 147.6626\n",
      " Running FID Round 18/20...\n",
      "Round 18 FID: 148.8303\n",
      " Running FID Round 19/20...\n",
      "Round 19 FID: 148.0432\n",
      " Running FID Round 20/20...\n",
      "Round 20 FID: 147.5679\n",
      "\n",
      " Final Average FID over 20 rounds: 147.9928 ± 0.8688\n"
     ]
    }
   ],
   "source": [
    "# Imports if needed\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "# Load pretrained InceptionV3 model\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).eval().cuda()\n",
    "\n",
    "# We only use features, so remove final fc layer\n",
    "inception_model.fc = torch.nn.Identity()\n",
    "\n",
    "# De-normalization helper (already defined if needed)\n",
    "def denormalize(imgs):\n",
    "    return (imgs + 1) / 2\n",
    "\n",
    "# Extract features from InceptionV3\n",
    "def get_inception_features(imgs):\n",
    "    device = next(inception_model.parameters()).device\n",
    "    \n",
    "    # Resize to 299x299\n",
    "    imgs_resized = F.interpolate(imgs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    features = inception_model(imgs_resized)\n",
    "    \n",
    "    return features.detach().cpu().numpy()\n",
    "\n",
    "# Function to calculate FID between two sets of activations\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1 = np.mean(act1, axis=0)\n",
    "    mu2 = np.mean(act2, axis=0)\n",
    "    sigma1 = np.cov(act1, rowvar=False)\n",
    "    sigma2 = np.cov(act2, rowvar=False)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Function to generate 1000 samples and compute FID\n",
    "def compute_fid_round(unet, vae, diffusion, real_images, num_samples=1000, latent_dim=256, timesteps=50):\n",
    "    device = next(unet.parameters()).device\n",
    "    samples = []\n",
    "    \n",
    "    batch_size = 100\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()\n",
    "        samples.append(decoded_imgs)\n",
    "    \n",
    "    generated_images = torch.cat(samples, dim=0)\n",
    "    generated_images_denorm = denormalize(generated_images)\n",
    "    real_images_denorm = denormalize(real_images)\n",
    "    \n",
    "    # Normalize for Inception\n",
    "    generated_images_incep = generated_images_denorm * 2 - 1  # back to [-1,1] for inception\n",
    "    real_images_incep = real_images_denorm * 2 - 1\n",
    "    \n",
    "    # Extract features\n",
    "    fake_acts = []\n",
    "    real_acts = []\n",
    "    batch_size = 100\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        fake_batch = generated_images_incep[i:i+batch_size].cuda()\n",
    "        real_batch = real_images_incep[i:i+batch_size].cuda()\n",
    "        \n",
    "        fake_acts.append(get_inception_features(fake_batch))\n",
    "        real_acts.append(get_inception_features(real_batch))\n",
    "    \n",
    "    fake_acts = np.concatenate(fake_acts, axis=0)\n",
    "    real_acts = np.concatenate(real_acts, axis=0)\n",
    "    \n",
    "    fid_score = calculate_fid(fake_acts, real_acts)\n",
    "    \n",
    "    return fid_score\n",
    "\n",
    "# Run FID for 20 rounds\n",
    "fid_all_rounds = []\n",
    "\n",
    "for round_num in range(20):\n",
    "    print(f\" Running FID Round {round_num+1}/20...\")\n",
    "    fid_score = compute_fid_round(\n",
    "        unet=unet_model,\n",
    "        vae=vae_model.cuda(),\n",
    "        diffusion=diffusion_obj,  # using 50 step diffusion\n",
    "        real_images=real_images,\n",
    "        timesteps=50\n",
    "    )\n",
    "    fid_all_rounds.append(fid_score)\n",
    "    print(f\"Round {round_num+1} FID: {fid_score:.4f}\")\n",
    "\n",
    "# Final Report\n",
    "fid_mean = sum(fid_all_rounds) / len(fid_all_rounds)\n",
    "fid_std = (sum([(x - fid_mean)**2 for x in fid_all_rounds]) / len(fid_all_rounds))**0.5\n",
    "\n",
    "print(f\"\\n Final Average FID over 20 rounds: {fid_mean:.4f} ± {fid_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c427eb1-6bb0-44bf-af4f-f96e6d374626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done: Saved 1000 images to 'generated_samples' folder.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Create save folder\n",
    "save_folder = \"generated_samples\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# CPU Sampling and Saving Function\n",
    "def generate_and_save_samples_cpu(unet, vae, diffusion, save_folder, num_samples=1000, latent_dim=256, timesteps=50):\n",
    "    device = torch.device(\"cpu\")  # Force CPU\n",
    "\n",
    "    # Move models to CPU\n",
    "    unet = unet.to(device)\n",
    "    vae = vae.to(device)\n",
    "\n",
    "    batch_size = 10  # Small batch to avoid memory pressure even on CPU\n",
    "    idx = 0\n",
    "\n",
    "    for _ in range(num_samples // batch_size):\n",
    "        x = torch.randn(batch_size, latent_dim).to(device)\n",
    "        for t in reversed(range(timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = diffusion.p_sample(unet, x, t_batch)\n",
    "        decoded_imgs = vae.decode(x).cpu()  # Output shape: (batch_size, 3, 32, 32)\n",
    "        decoded_imgs = (decoded_imgs + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "\n",
    "        # Save each image individually\n",
    "        for img in decoded_imgs:\n",
    "            save_path = os.path.join(save_folder, f\"{idx:04d}.png\")\n",
    "            vutils.save_image(img, save_path)\n",
    "            idx += 1\n",
    "\n",
    "    print(f\"\\n Done: Saved {idx} images to '{save_folder}' folder.\")\n",
    "\n",
    "# Example usage:\n",
    "generate_and_save_samples_cpu(\n",
    "    unet=unet_model,\n",
    "    vae=vae_model,\n",
    "    diffusion=diffusion_obj,\n",
    "    save_folder=save_folder,\n",
    "    num_samples=1000,       \n",
    "    latent_dim=256,\n",
    "    timesteps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7247fed-1805-40a9-8699-296fd7e7f8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchmetrics in ./.local/lib/python3.9/site-packages (0.6.0)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in ./.local/lib/python3.9/site-packages (from torchmetrics) (1.24.4)\n",
      "Requirement already satisfied: packaging>17.1 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.9/site-packages (from torchmetrics) (2.5.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in ./.local/lib/python3.9/site-packages (from torchmetrics) (0.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (53.0.0)\n",
      "Requirement already satisfied: typing_extensions in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (3.7.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torchmetrics\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 0.6.0\n",
      "    Not uninstalling torchmetrics at /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages, outside environment /apps/project/ondemand/app_jupyter/4.1.5\n",
      "    Can't uninstall 'torchmetrics'. No files were found to uninstall.\n",
      "Successfully installed torchmetrics-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36a8f8bd-5f20-4a53-8c04-1696ff9378da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading all saved images...\n",
      " Loaded 1000 images.\n",
      "\n",
      " Calculating Inception Score for 20 rounds...\n",
      " Round 1: Inception Score = 1.1485\n",
      " Round 2: Inception Score = 1.1480\n",
      " Round 3: Inception Score = 1.1499\n",
      " Round 4: Inception Score = 1.1468\n",
      " Round 5: Inception Score = 1.1473\n",
      " Round 6: Inception Score = 1.1476\n",
      " Round 7: Inception Score = 1.1530\n",
      " Round 8: Inception Score = 1.1538\n",
      " Round 9: Inception Score = 1.1494\n",
      " Round 10: Inception Score = 1.1489\n",
      " Round 11: Inception Score = 1.1518\n",
      " Round 12: Inception Score = 1.1500\n",
      " Round 13: Inception Score = 1.1483\n",
      " Round 14: Inception Score = 1.1364\n",
      " Round 15: Inception Score = 1.1387\n",
      " Round 16: Inception Score = 1.1424\n",
      " Round 17: Inception Score = 1.1461\n",
      " Round 18: Inception Score = 1.1440\n",
      " Round 19: Inception Score = 1.1398\n",
      " Round 20: Inception Score = 1.1469\n",
      "\n",
      " Final Inception Score over 20 rounds: 1.1469 ± 0.0045\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Folder where saved images are\n",
    "save_folder = \"generated_samples\"\n",
    "\n",
    "# Define Transform: Resize to 299x299\n",
    "transform = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom dataset\n",
    "class SimpleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        img = default_loader(img_path)  # PIL image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load dataset\n",
    "dataset = SimpleImageDataset(save_folder, transform=transform)\n",
    "\n",
    "# Load all images into memory\n",
    "print(\" Loading all saved images...\")\n",
    "all_images = []\n",
    "for idx in range(len(dataset)):\n",
    "    all_images.append(dataset[idx])\n",
    "\n",
    "all_images = torch.stack(all_images, dim=0)\n",
    "print(f\" Loaded {all_images.shape[0]} images.\")\n",
    "\n",
    "# Load InceptionV3 model (CPU safe)\n",
    "inception_model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception_model.fc = torch.nn.Identity()  # Remove classification head\n",
    "inception_model.eval()\n",
    "inception_model = inception_model.cpu()\n",
    "\n",
    "# Function to calculate IS manually\n",
    "def calculate_inception_score(preds, splits=10):\n",
    "    preds = np.asarray(preds)\n",
    "    scores = []\n",
    "    split_size = preds.shape[0] // splits\n",
    "    for i in range(splits):\n",
    "        part = preds[i * split_size: (i + 1) * split_size]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, axis=1))\n",
    "        scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# 20 Rounds Inception Score\n",
    "round_scores = []\n",
    "num_rounds = 20\n",
    "num_samples_per_round = 500\n",
    "\n",
    "print(\"\\n Calculating Inception Score for 20 rounds...\")\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    # Randomly pick 500 images for each round\n",
    "    indices = random.sample(range(len(all_images)), num_samples_per_round)\n",
    "    selected_images = all_images[indices]\n",
    "\n",
    "    preds = []\n",
    "    batch_size = 10  # Safe for CPU\n",
    "    for i in range(0, selected_images.shape[0], batch_size):\n",
    "        batch = selected_images[i:i+batch_size]\n",
    "        batch_resized = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        logits = inception_model(batch_resized)\n",
    "        preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    mean_is, std_is = calculate_inception_score(preds, splits=10)\n",
    "    round_scores.append(mean_is)\n",
    "    print(f\" Round {round_num+1}: Inception Score = {mean_is:.4f}\")\n",
    "\n",
    "# Final Results\n",
    "is_mean = sum(round_scores) / len(round_scores)\n",
    "is_std = (sum([(x - is_mean)**2 for x in round_scores]) / len(round_scores))**0.5\n",
    "\n",
    "print(f\"\\n Final Inception Score over {num_rounds} rounds: {is_mean:.4f} ± {is_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "766e3636-c11b-4001-89fb-d7ec691e4f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading all saved images...\n",
      " Loaded 1000 images.\n",
      "\n",
      " Calculating KID for 20 rounds...\n",
      " Round 1: KID = -0.000585\n",
      " Round 2: KID = -0.000536\n",
      " Round 3: KID = -0.000653\n",
      " Round 4: KID = -0.000454\n",
      " Round 5: KID = -0.000578\n",
      " Round 6: KID = -0.000441\n",
      " Round 7: KID = -0.000464\n",
      " Round 8: KID = -0.000629\n",
      " Round 9: KID = -0.000606\n",
      " Round 10: KID = -0.000501\n",
      " Round 11: KID = -0.000512\n",
      " Round 12: KID = -0.000537\n",
      " Round 13: KID = -0.000312\n",
      " Round 14: KID = -0.000565\n",
      " Round 15: KID = -0.000490\n",
      " Round 16: KID = -0.000408\n",
      " Round 17: KID = -0.000526\n",
      " Round 18: KID = -0.000252\n",
      " Round 19: KID = -0.000445\n",
      " Round 20: KID = -0.000439\n",
      "\n",
      " Final KID over 20 rounds: -0.000497 ± 0.000098\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "\n",
    "# Folder where saved images are\n",
    "save_folder = \"generated_samples\"\n",
    "\n",
    "# Define Transform: Resize to 299x299\n",
    "transform = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom dataset\n",
    "class SimpleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        img = default_loader(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load dataset\n",
    "dataset = SimpleImageDataset(save_folder, transform=transform)\n",
    "\n",
    "# Load all images into memory\n",
    "print(\" Loading all saved images...\")\n",
    "all_images = []\n",
    "for idx in range(len(dataset)):\n",
    "    all_images.append(dataset[idx])\n",
    "\n",
    "all_images = torch.stack(all_images, dim=0)\n",
    "print(f\" Loaded {all_images.shape[0]} images.\")\n",
    "\n",
    "# Load InceptionV3 model (CPU safe)\n",
    "inception_model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception_model.fc = torch.nn.Identity()\n",
    "inception_model.eval()\n",
    "inception_model = inception_model.cpu()\n",
    "\n",
    "real_images = all_images\n",
    "\n",
    "# Feature extractor function\n",
    "def extract_features(imgs, batch_size=10):\n",
    "    features = []\n",
    "    for i in range(0, imgs.shape[0], batch_size):\n",
    "        batch = imgs[i:i+batch_size]\n",
    "        resized = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        feats = inception_model(resized)\n",
    "        features.append(feats.detach().cpu())\n",
    "    return torch.cat(features, dim=0)\n",
    "\n",
    "# 20 Rounds KID\n",
    "round_scores = []\n",
    "num_rounds = 20\n",
    "num_samples_per_round = 500\n",
    "\n",
    "print(\"\\n Calculating KID for 20 rounds...\")\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    # Randomly pick 500 fake images and 500 real images\n",
    "    fake_indices = random.sample(range(len(all_images)), num_samples_per_round)\n",
    "    real_indices = random.sample(range(len(real_images)), num_samples_per_round)\n",
    "\n",
    "    fake_imgs = all_images[fake_indices]\n",
    "    real_imgs = real_images[real_indices]\n",
    "\n",
    "    # Extract Inception features\n",
    "    fake_feats = extract_features(fake_imgs)\n",
    "    real_feats = extract_features(real_imgs)\n",
    "\n",
    "    # Compute polynomial kernel\n",
    "    k_xx = polynomial_kernel(fake_feats.numpy(), fake_feats.numpy(), degree=3, coef0=1)\n",
    "    k_yy = polynomial_kernel(real_feats.numpy(), real_feats.numpy(), degree=3, coef0=1)\n",
    "    k_xy = polynomial_kernel(fake_feats.numpy(), real_feats.numpy(), degree=3, coef0=1)\n",
    "\n",
    "    m = k_xx.shape[0]  # number of samples\n",
    "\n",
    "    # KID formula\n",
    "    kid = (k_xx.sum() - np.trace(k_xx)) / (m * (m-1)) + \\\n",
    "          (k_yy.sum() - np.trace(k_yy)) / (m * (m-1)) - \\\n",
    "          2 * k_xy.mean()\n",
    "\n",
    "    round_scores.append(kid)\n",
    "    print(f\" Round {round_num+1}: KID = {kid:.6f}\")\n",
    "\n",
    "# Final Results\n",
    "kid_mean = sum(round_scores) / len(round_scores)\n",
    "kid_std = (sum([(x - kid_mean)**2 for x in round_scores]) / len(round_scores))**0.5\n",
    "\n",
    "print(f\"\\n Final KID over {num_rounds} rounds: {kid_mean:.6f} ± {kid_std:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0276c837-ad4f-4e63-a559-42aa4d3ac751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lpips in ./.local/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in ./.local/lib/python3.9/site-packages (from lpips) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in ./.local/lib/python3.9/site-packages (from lpips) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in ./.local/lib/python3.9/site-packages (from lpips) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in ./.local/lib/python3.9/site-packages (from lpips) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in ./.local/lib/python3.9/site-packages (from lpips) (4.67.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib64/python3.9/site-packages (from torchvision>=0.2.1->lpips) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e021207-9562-4f08-af95-0a725a259a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading all saved images...\n",
      " Loaded 1000 images.\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "\n",
      " Calculating LPIPS for 20 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1: LPIPS = 0.185491\n",
      " Round 2: LPIPS = 0.187324\n",
      " Round 3: LPIPS = 0.185825\n",
      " Round 4: LPIPS = 0.186252\n",
      " Round 5: LPIPS = 0.187991\n",
      " Round 6: LPIPS = 0.184988\n",
      " Round 7: LPIPS = 0.190775\n",
      " Round 8: LPIPS = 0.184498\n",
      " Round 9: LPIPS = 0.190075\n",
      " Round 10: LPIPS = 0.189889\n",
      " Round 11: LPIPS = 0.185690\n",
      " Round 12: LPIPS = 0.184030\n",
      " Round 13: LPIPS = 0.185169\n",
      " Round 14: LPIPS = 0.188202\n",
      " Round 15: LPIPS = 0.182661\n",
      " Round 16: LPIPS = 0.192573\n",
      " Round 17: LPIPS = 0.192418\n",
      " Round 18: LPIPS = 0.188600\n",
      " Round 19: LPIPS = 0.189294\n",
      " Round 20: LPIPS = 0.188465\n",
      "\n",
      " Final LPIPS over 20 rounds: 0.187510 ± 0.002711\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import lpips\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Folder where saved images are\n",
    "save_folder = \"generated_samples\"\n",
    "\n",
    "# Define Transform\n",
    "transform = T.Compose([\n",
    "    T.Resize((32, 32)),  # LPIPS expects 32x32 if your model is trained like CIFAR-10\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom dataset\n",
    "class SimpleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        img = default_loader(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load dataset\n",
    "dataset = SimpleImageDataset(save_folder, transform=transform)\n",
    "\n",
    "# Load all images into memory\n",
    "print(\" Loading all saved images...\")\n",
    "all_images = []\n",
    "for idx in range(len(dataset)):\n",
    "    all_images.append(dataset[idx])\n",
    "\n",
    "all_images = torch.stack(all_images, dim=0)\n",
    "print(f\" Loaded {all_images.shape[0]} images.\")\n",
    "\n",
    "# Load LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')  # or use 'vgg'\n",
    "lpips_model.eval()\n",
    "lpips_model = lpips_model.cpu()\n",
    "\n",
    "# Assume real_images are same as all_images (for demonstration)  \n",
    "real_images = all_images\n",
    "\n",
    "# 20 Rounds LPIPS\n",
    "round_scores = []\n",
    "num_rounds = 20\n",
    "num_samples_per_round = 500\n",
    "\n",
    "print(\"\\n Calculating LPIPS for 20 rounds...\")\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    # Randomly pick 500 fake and 500 real images\n",
    "    fake_indices = random.sample(range(len(all_images)), num_samples_per_round)\n",
    "    real_indices = random.sample(range(len(real_images)), num_samples_per_round)\n",
    "\n",
    "    fake_imgs = all_images[fake_indices]\n",
    "    real_imgs = real_images[real_indices]\n",
    "\n",
    "    lpips_scores = []\n",
    "    for i in range(num_samples_per_round):\n",
    "        fake = fake_imgs[i].unsqueeze(0) * 2 - 1  # LPIPS expects [-1, 1] images\n",
    "        real = real_imgs[i].unsqueeze(0) * 2 - 1\n",
    "        dist = lpips_model(fake, real)\n",
    "        lpips_scores.append(dist.item())\n",
    "\n",
    "    avg_lpips = sum(lpips_scores) / len(lpips_scores)\n",
    "    round_scores.append(avg_lpips)\n",
    "    print(f\" Round {round_num+1}: LPIPS = {avg_lpips:.6f}\")\n",
    "\n",
    "# Final Results\n",
    "lpips_mean = sum(round_scores) / len(round_scores)\n",
    "lpips_std = (sum([(x - lpips_mean)**2 for x in round_scores]) / len(round_scores))**0.5\n",
    "\n",
    "print(f\"\\n Final LPIPS over {num_rounds} rounds: {lpips_mean:.6f} ± {lpips_std:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b51f7fc-2898-49b4-a1c3-7aff9afe3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloading DIV2K_train_HR.zip...\n",
      " Downloaded DIV2K_train_HR.zip\n",
      " Extracting DIV2K_train_HR.zip...\n",
      " Extracted to ./DIV2K_train_HR\n",
      "Files inside ./DIV2K_train_HR: 1 images ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# URL to DIV2K Train High Resolution images\n",
    "div2k_url = \"https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
    "zip_filename = \"DIV2K_train_HR.zip\"\n",
    "extract_folder = \"./DIV2K_train_HR\"\n",
    "\n",
    "# Step 1: Download the ZIP file\n",
    "if not os.path.exists(zip_filename):\n",
    "    print(f\" Downloading DIV2K_train_HR.zip...\")\n",
    "    response = requests.get(div2k_url, stream=True)\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(f\" Downloaded {zip_filename}\")\n",
    "else:\n",
    "    print(f\" {zip_filename} already downloaded.\")\n",
    "\n",
    "# Step 2: Extract the ZIP file\n",
    "if not os.path.exists(extract_folder):\n",
    "    print(f\" Extracting {zip_filename}...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "    print(f\" Extracted to {extract_folder}\")\n",
    "else:\n",
    "    print(f\" {extract_folder} already extracted.\")\n",
    "\n",
    "# Final check\n",
    "print(f\"Files inside {extract_folder}: {len(os.listdir(extract_folder))} images ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f36fcea-21e2-4a91-b25a-b7501ee4f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DIV2K prepared: 720 training images, 80 validation images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# DIV2K images are already extracted\n",
    "div2k_root = \"./DIV2K_train_HR\"\n",
    "\n",
    "# Define 32x32 Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),   # Resize to 32x32\n",
    "    transforms.ToTensor(),         # Convert to Tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(root_dir, fname)\n",
    "            for fname in os.listdir(root_dir)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load DIV2K\n",
    "div2k_dataset = DIV2KDataset(root_dir=div2k_root, transform=transform)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * len(div2k_dataset))\n",
    "val_size = len(div2k_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(div2k_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\" DIV2K prepared: {len(train_dataset)} training images, {len(val_dataset)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1529ca84-1289-4a25-bb4e-dfd3e8c9ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),   # 32 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # 16 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # 8 -> 4\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 4 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 8 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),     # 16 -> 32\n",
    "            nn.Tanh()  # output between [-1,1]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 256, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "033a3cb2-130e-4966-bd28-b66f0d6d73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b63a377-ef13-4ec3-ab30-f2b0d2162df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.261157\n",
      "Epoch 2/100 - Train Loss: 0.243455\n",
      "Epoch 3/100 - Train Loss: 0.239531\n",
      "Epoch 4/100 - Train Loss: 0.237904\n",
      "Epoch 5/100 - Train Loss: 0.235645\n",
      "Epoch 6/100 - Train Loss: 0.234998\n",
      "Epoch 7/100 - Train Loss: 0.235900\n",
      "Epoch 8/100 - Train Loss: 0.233780\n",
      "Epoch 9/100 - Train Loss: 0.235825\n",
      "Epoch 10/100 - Train Loss: 0.232482\n",
      "Epoch 11/100 - Train Loss: 0.231835\n",
      "Epoch 12/100 - Train Loss: 0.233456\n",
      "Epoch 13/100 - Train Loss: 0.228644\n",
      "Epoch 14/100 - Train Loss: 0.235599\n",
      "Epoch 15/100 - Train Loss: 0.231847\n",
      "Epoch 16/100 - Train Loss: 0.229637\n",
      "Epoch 17/100 - Train Loss: 0.233150\n",
      "Epoch 18/100 - Train Loss: 0.230803\n",
      "Epoch 19/100 - Train Loss: 0.230319\n",
      "Epoch 20/100 - Train Loss: 0.230293\n",
      "Epoch 21/100 - Train Loss: 0.234648\n",
      "Epoch 22/100 - Train Loss: 0.231803\n",
      "Epoch 23/100 - Train Loss: 0.229762\n",
      "Epoch 24/100 - Train Loss: 0.234405\n",
      "Epoch 25/100 - Train Loss: 0.231008\n",
      "Epoch 26/100 - Train Loss: 0.227943\n",
      "Epoch 27/100 - Train Loss: 0.230624\n",
      "Epoch 28/100 - Train Loss: 0.230035\n",
      "Epoch 29/100 - Train Loss: 0.230741\n",
      "Epoch 30/100 - Train Loss: 0.230097\n",
      "Epoch 31/100 - Train Loss: 0.229459\n",
      "Epoch 32/100 - Train Loss: 0.228914\n",
      "Epoch 33/100 - Train Loss: 0.228572\n",
      "Epoch 34/100 - Train Loss: 0.230826\n",
      "Epoch 35/100 - Train Loss: 0.227839\n",
      "Epoch 36/100 - Train Loss: 0.227383\n",
      "Epoch 37/100 - Train Loss: 0.227247\n",
      "Epoch 38/100 - Train Loss: 0.230842\n",
      "Epoch 39/100 - Train Loss: 0.227632\n",
      "Epoch 40/100 - Train Loss: 0.226790\n",
      "Epoch 41/100 - Train Loss: 0.231523\n",
      "Epoch 42/100 - Train Loss: 0.231111\n",
      "Epoch 43/100 - Train Loss: 0.227379\n",
      "Epoch 44/100 - Train Loss: 0.229978\n",
      "Epoch 45/100 - Train Loss: 0.230013\n",
      "Epoch 46/100 - Train Loss: 0.226266\n",
      "Epoch 47/100 - Train Loss: 0.230003\n",
      "Epoch 48/100 - Train Loss: 0.229669\n",
      "Epoch 49/100 - Train Loss: 0.227709\n",
      "Epoch 50/100 - Train Loss: 0.227313\n",
      "Epoch 51/100 - Train Loss: 0.228237\n",
      "Epoch 52/100 - Train Loss: 0.233633\n",
      "Epoch 53/100 - Train Loss: 0.230057\n",
      "Epoch 54/100 - Train Loss: 0.231703\n",
      "Epoch 55/100 - Train Loss: 0.229628\n",
      "Epoch 56/100 - Train Loss: 0.226465\n",
      "Epoch 57/100 - Train Loss: 0.226715\n",
      "Epoch 58/100 - Train Loss: 0.229220\n",
      "Epoch 59/100 - Train Loss: 0.227622\n",
      "Epoch 60/100 - Train Loss: 0.230912\n",
      "Epoch 61/100 - Train Loss: 0.229983\n",
      "Epoch 62/100 - Train Loss: 0.227200\n",
      "Epoch 63/100 - Train Loss: 0.231076\n",
      "Epoch 64/100 - Train Loss: 0.225492\n",
      "Epoch 65/100 - Train Loss: 0.229991\n",
      "Epoch 66/100 - Train Loss: 0.226539\n",
      "Epoch 67/100 - Train Loss: 0.227642\n",
      "Epoch 68/100 - Train Loss: 0.228531\n",
      "Epoch 69/100 - Train Loss: 0.228516\n",
      "Epoch 70/100 - Train Loss: 0.229684\n",
      "Epoch 71/100 - Train Loss: 0.225902\n",
      "Epoch 72/100 - Train Loss: 0.230106\n",
      "Epoch 73/100 - Train Loss: 0.227234\n",
      "Epoch 74/100 - Train Loss: 0.228840\n",
      "Epoch 75/100 - Train Loss: 0.227349\n",
      "Epoch 76/100 - Train Loss: 0.228521\n",
      "Epoch 77/100 - Train Loss: 0.230156\n",
      "Epoch 78/100 - Train Loss: 0.226570\n",
      "Epoch 79/100 - Train Loss: 0.228568\n",
      "Epoch 80/100 - Train Loss: 0.227055\n",
      "Epoch 81/100 - Train Loss: 0.228032\n",
      "Epoch 82/100 - Train Loss: 0.227838\n",
      "Epoch 83/100 - Train Loss: 0.226593\n",
      "Epoch 84/100 - Train Loss: 0.226176\n",
      "Epoch 85/100 - Train Loss: 0.226995\n",
      "Epoch 86/100 - Train Loss: 0.230430\n",
      "Epoch 87/100 - Train Loss: 0.227470\n",
      "Epoch 88/100 - Train Loss: 0.228558\n",
      "Epoch 89/100 - Train Loss: 0.228772\n",
      "Epoch 90/100 - Train Loss: 0.227170\n",
      "Epoch 91/100 - Train Loss: 0.227724\n",
      "Epoch 92/100 - Train Loss: 0.226138\n",
      "Epoch 93/100 - Train Loss: 0.224647\n",
      "Epoch 94/100 - Train Loss: 0.228612\n",
      "Epoch 95/100 - Train Loss: 0.228251\n",
      "Epoch 96/100 - Train Loss: 0.229044\n",
      "Epoch 97/100 - Train Loss: 0.226822\n",
      "Epoch 98/100 - Train Loss: 0.226522\n",
      "Epoch 99/100 - Train Loss: 0.226905\n",
      "Epoch 100/100 - Train Loss: 0.229375\n",
      " VAE model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 256\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "vae_model = VAE(latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    vae_model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            batch = batch[0]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(batch)\n",
    "        loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(vae_model.state_dict(), \"vae_div2k_32.pth\")\n",
    "print(\" VAE model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1579a72d-59e0-4cec-bdb6-0965f1f9c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, latent_dim=256, time_emb_dim=256):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + time_emb_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).float()  # (batch, 1)\n",
    "        t_emb = self.time_mlp(t)     # (batch, time_emb_dim)\n",
    "        x = torch.cat([x, t_emb], dim=-1)  # Concatenate time embedding\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c90be71-132c-4ae1-bc74-562c4e8324c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, timesteps=1000, device=device):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        self.beta = torch.linspace(1e-4, 0.02, timesteps).to(device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0).to(device)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t]).unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t]).unsqueeze(1)\n",
    "\n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        sqrt_recip_alpha_hat = torch.sqrt(1.0 / self.alpha_hat[t]).unsqueeze(1)\n",
    "        sqrt_recipm1_alpha_hat = torch.sqrt(1.0 / self.alpha_hat[t] - 1).unsqueeze(1)\n",
    "\n",
    "        return sqrt_recip_alpha_hat * x_t - sqrt_recipm1_alpha_hat * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94f29c7e-e277-4718-8892-28ca2e7284bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.660196/ipykernel_3271377/2653609718.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Diffusion Loss: 16.046989\n",
      "Epoch 2/100 - Diffusion Loss: 1.435262\n",
      "Epoch 3/100 - Diffusion Loss: 1.075751\n",
      "Epoch 4/100 - Diffusion Loss: 1.025852\n",
      "Epoch 5/100 - Diffusion Loss: 1.010772\n",
      "Epoch 6/100 - Diffusion Loss: 1.007144\n",
      "Epoch 7/100 - Diffusion Loss: 1.004500\n",
      "Epoch 8/100 - Diffusion Loss: 1.003654\n",
      "Epoch 9/100 - Diffusion Loss: 1.004325\n",
      "Epoch 10/100 - Diffusion Loss: 0.999274\n",
      "Epoch 11/100 - Diffusion Loss: 1.006779\n",
      "Epoch 12/100 - Diffusion Loss: 0.996199\n",
      "Epoch 13/100 - Diffusion Loss: 1.001539\n",
      "Epoch 14/100 - Diffusion Loss: 1.003540\n",
      "Epoch 15/100 - Diffusion Loss: 1.002675\n",
      "Epoch 16/100 - Diffusion Loss: 0.998025\n",
      "Epoch 17/100 - Diffusion Loss: 0.998680\n",
      "Epoch 18/100 - Diffusion Loss: 0.998301\n",
      "Epoch 19/100 - Diffusion Loss: 0.996731\n",
      "Epoch 20/100 - Diffusion Loss: 1.000722\n",
      "Epoch 21/100 - Diffusion Loss: 1.000914\n",
      "Epoch 22/100 - Diffusion Loss: 0.997173\n",
      "Epoch 23/100 - Diffusion Loss: 0.997635\n",
      "Epoch 24/100 - Diffusion Loss: 0.997249\n",
      "Epoch 25/100 - Diffusion Loss: 0.998652\n",
      "Epoch 26/100 - Diffusion Loss: 1.000257\n",
      "Epoch 27/100 - Diffusion Loss: 0.997196\n",
      "Epoch 28/100 - Diffusion Loss: 0.996743\n",
      "Epoch 29/100 - Diffusion Loss: 1.003599\n",
      "Epoch 30/100 - Diffusion Loss: 1.004755\n",
      "Epoch 31/100 - Diffusion Loss: 1.000223\n",
      "Epoch 32/100 - Diffusion Loss: 1.004780\n",
      "Epoch 33/100 - Diffusion Loss: 1.001975\n",
      "Epoch 34/100 - Diffusion Loss: 1.002956\n",
      "Epoch 35/100 - Diffusion Loss: 1.005353\n",
      "Epoch 36/100 - Diffusion Loss: 0.995575\n",
      "Epoch 37/100 - Diffusion Loss: 1.003486\n",
      "Epoch 38/100 - Diffusion Loss: 0.992610\n",
      "Epoch 39/100 - Diffusion Loss: 0.996478\n",
      "Epoch 40/100 - Diffusion Loss: 0.998319\n",
      "Epoch 41/100 - Diffusion Loss: 1.002103\n",
      "Epoch 42/100 - Diffusion Loss: 0.997536\n",
      "Epoch 43/100 - Diffusion Loss: 1.001981\n",
      "Epoch 44/100 - Diffusion Loss: 1.004276\n",
      "Epoch 45/100 - Diffusion Loss: 1.001947\n",
      "Epoch 46/100 - Diffusion Loss: 0.997702\n",
      "Epoch 47/100 - Diffusion Loss: 1.005765\n",
      "Epoch 48/100 - Diffusion Loss: 1.005337\n",
      "Epoch 49/100 - Diffusion Loss: 1.002997\n",
      "Epoch 50/100 - Diffusion Loss: 0.998256\n",
      "Epoch 51/100 - Diffusion Loss: 0.994101\n",
      "Epoch 52/100 - Diffusion Loss: 1.002840\n",
      "Epoch 53/100 - Diffusion Loss: 1.001684\n",
      "Epoch 54/100 - Diffusion Loss: 0.998011\n",
      "Epoch 55/100 - Diffusion Loss: 0.997609\n",
      "Epoch 56/100 - Diffusion Loss: 0.998846\n",
      "Epoch 57/100 - Diffusion Loss: 1.003069\n",
      "Epoch 58/100 - Diffusion Loss: 1.000421\n",
      "Epoch 59/100 - Diffusion Loss: 1.000010\n",
      "Epoch 60/100 - Diffusion Loss: 1.001288\n",
      "Epoch 61/100 - Diffusion Loss: 0.996736\n",
      "Epoch 62/100 - Diffusion Loss: 0.994474\n",
      "Epoch 63/100 - Diffusion Loss: 0.999096\n",
      "Epoch 64/100 - Diffusion Loss: 0.998198\n",
      "Epoch 65/100 - Diffusion Loss: 0.995988\n",
      "Epoch 66/100 - Diffusion Loss: 0.999156\n",
      "Epoch 67/100 - Diffusion Loss: 0.995692\n",
      "Epoch 68/100 - Diffusion Loss: 0.996162\n",
      "Epoch 69/100 - Diffusion Loss: 0.994041\n",
      "Epoch 70/100 - Diffusion Loss: 1.000170\n",
      "Epoch 71/100 - Diffusion Loss: 0.995791\n",
      "Epoch 72/100 - Diffusion Loss: 0.998117\n",
      "Epoch 73/100 - Diffusion Loss: 1.007683\n",
      "Epoch 74/100 - Diffusion Loss: 1.000277\n",
      "Epoch 75/100 - Diffusion Loss: 1.003728\n",
      "Epoch 76/100 - Diffusion Loss: 1.002139\n",
      "Epoch 77/100 - Diffusion Loss: 0.996485\n",
      "Epoch 78/100 - Diffusion Loss: 1.000812\n",
      "Epoch 79/100 - Diffusion Loss: 1.002892\n",
      "Epoch 80/100 - Diffusion Loss: 0.998741\n",
      "Epoch 81/100 - Diffusion Loss: 1.003774\n",
      "Epoch 82/100 - Diffusion Loss: 0.995834\n",
      "Epoch 83/100 - Diffusion Loss: 1.005067\n",
      "Epoch 84/100 - Diffusion Loss: 1.005434\n",
      "Epoch 85/100 - Diffusion Loss: 1.001937\n",
      "Epoch 86/100 - Diffusion Loss: 0.999169\n",
      "Epoch 87/100 - Diffusion Loss: 1.002923\n",
      "Epoch 88/100 - Diffusion Loss: 1.007935\n",
      "Epoch 89/100 - Diffusion Loss: 0.995054\n",
      "Epoch 90/100 - Diffusion Loss: 0.999719\n",
      "Epoch 91/100 - Diffusion Loss: 0.997282\n",
      "Epoch 92/100 - Diffusion Loss: 0.995409\n",
      "Epoch 93/100 - Diffusion Loss: 0.999823\n",
      "Epoch 94/100 - Diffusion Loss: 1.002758\n",
      "Epoch 95/100 - Diffusion Loss: 1.004609\n",
      "Epoch 96/100 - Diffusion Loss: 0.997222\n",
      "Epoch 97/100 - Diffusion Loss: 1.002216\n",
      "Epoch 98/100 - Diffusion Loss: 0.995403\n",
      "Epoch 99/100 - Diffusion Loss: 1.007262\n",
      "Epoch 100/100 - Diffusion Loss: 0.998186\n",
      " UNet Diffusion model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 1000\n",
    "latent_dim = 256\n",
    "num_epochs = 100\n",
    "learning_rate = 2e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Models\n",
    "unet_model = UNet(latent_dim=latent_dim).to(device)\n",
    "diffusion = Diffusion(timesteps=timesteps)\n",
    "\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load trained VAE\n",
    "vae_model = VAE(latent_dim=latent_dim).to(device)\n",
    "vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n",
    "vae_model.eval()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    unet_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            batch = batch[0]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = vae_model.encode(batch)\n",
    "            z = vae_model.reparameterize(mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, timesteps, (z.size(0),), device=z.device).long()\n",
    "        noise = torch.randn_like(z)\n",
    "        z_noisy = diffusion.q_sample(z, t, noise)\n",
    "\n",
    "        noise_pred = unet_model(z_noisy, t)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Diffusion Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Save UNet\n",
    "torch.save(unet_model.state_dict(), \"unet_diffusion_div2k_32.pth\")\n",
    "print(\" UNet Diffusion model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "745c2d04-f1c1-4089-84ec-2f56fc6def2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.660196/ipykernel_3271377/2466190201.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n",
      "/tmp/slurmtmp.660196/ipykernel_3271377/2466190201.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet_model.load_state_dict(torch.load(\"unet_diffusion_div2k_32.pth\"))\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/matplotlib/cm.py:494: RuntimeWarning: invalid value encountered in cast\n",
      "  xx = (xx * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload models (in case you restarted kernel)\n",
    "vae_model = VAE(latent_dim=256).to(device)\n",
    "vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n",
    "vae_model.eval()\n",
    "\n",
    "unet_model = UNet(latent_dim=256).to(device)\n",
    "unet_model.load_state_dict(torch.load(\"unet_diffusion_div2k_32.pth\"))\n",
    "unet_model.eval()\n",
    "\n",
    "diffusion = Diffusion(timesteps=1000, device=device)\n",
    "\n",
    "# Sampling function\n",
    "def sample_ldm(unet, vae, diffusion, num_samples=8, latent_dim=256, timesteps=1000):\n",
    "    device = next(unet.parameters()).device\n",
    "\n",
    "    x = torch.randn(num_samples, latent_dim).to(device)\n",
    "    \n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(x, t_batch)\n",
    "            sqrt_recip_alpha_hat = torch.sqrt(1.0 / diffusion.alpha_hat[t_batch]).unsqueeze(1)\n",
    "            sqrt_recipm1_alpha_hat = torch.sqrt(1.0 / diffusion.alpha_hat[t_batch] - 1).unsqueeze(1)\n",
    "            x = sqrt_recip_alpha_hat * (x - sqrt_recipm1_alpha_hat * noise_pred)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                beta = diffusion.beta[t_batch].unsqueeze(1)\n",
    "                x = x + torch.sqrt(beta) * noise\n",
    "\n",
    "    # Decode latent vectors into images\n",
    "    with torch.no_grad():\n",
    "        decoded_imgs = vae.decode(x)\n",
    "        decoded_imgs = (decoded_imgs + 1) / 2  # Denormalize to [0,1]\n",
    "    \n",
    "    return decoded_imgs\n",
    "\n",
    "# Generate samples\n",
    "generated_images = sample_ldm(\n",
    "    unet=unet_model,\n",
    "    vae=vae_model,\n",
    "    diffusion=diffusion,\n",
    "    num_samples=8,\n",
    "    latent_dim=256,\n",
    "    timesteps=1000\n",
    ")\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(20, 5))\n",
    "for idx, img in enumerate(generated_images):\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6232753-1dfe-4bf8-afd8-dfeef5cd1bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.660196/ipykernel_3271377/46766431.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n",
      "/tmp/slurmtmp.660196/ipykernel_3271377/46766431.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet_model.load_state_dict(torch.load(\"unet_diffusion_div2k_32.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting sample generation (1000 images)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:58<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All 1000 samples saved to ./generated_div2k_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload trained models (if needed)\n",
    "vae_model = VAE(latent_dim=256).to(device)\n",
    "vae_model.load_state_dict(torch.load(\"vae_div2k_32.pth\"))\n",
    "vae_model.eval()\n",
    "\n",
    "unet_model = UNet(latent_dim=256).to(device)\n",
    "unet_model.load_state_dict(torch.load(\"unet_diffusion_div2k_32.pth\"))\n",
    "unet_model.eval()\n",
    "\n",
    "diffusion = Diffusion(timesteps=1000, device=device)\n",
    "\n",
    "# Folder to save samples\n",
    "save_folder = \"./generated_div2k_32\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Sampling function\n",
    "def sample_ldm(unet, vae, diffusion, num_samples=8, latent_dim=256, timesteps=1000):\n",
    "    device = next(unet.parameters()).device\n",
    "    x = torch.randn(num_samples, latent_dim).to(device)\n",
    "    \n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(x, t_batch)\n",
    "            sqrt_recip_alpha_hat = torch.sqrt(1.0 / diffusion.alpha_hat[t_batch]).unsqueeze(1)\n",
    "            sqrt_recipm1_alpha_hat = torch.sqrt(1.0 / diffusion.alpha_hat[t_batch] - 1).unsqueeze(1)\n",
    "            x = sqrt_recip_alpha_hat * (x - sqrt_recipm1_alpha_hat * noise_pred)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                beta = diffusion.beta[t_batch].unsqueeze(1)\n",
    "                x = x + torch.sqrt(beta) * noise\n",
    "\n",
    "    # Decode latent vectors into images\n",
    "    with torch.no_grad():\n",
    "        decoded_imgs = vae.decode(x)\n",
    "        decoded_imgs = (decoded_imgs + 1) / 2  # Denormalize to [0,1]\n",
    "    \n",
    "    return decoded_imgs\n",
    "\n",
    "# Generate and save 1000 samples\n",
    "batch_size = 8\n",
    "num_samples = 1000\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "print(f\" Starting sample generation ({num_samples} images)...\")\n",
    "\n",
    "sample_idx = 0\n",
    "for _ in tqdm(range(num_batches)):\n",
    "    samples = sample_ldm(unet_model, vae_model, diffusion, num_samples=batch_size, latent_dim=256, timesteps=1000)\n",
    "    for img in samples:\n",
    "        img_path = os.path.join(save_folder, f\"sample_{sample_idx:04d}.png\")\n",
    "        vutils.save_image(img, img_path, normalize=True)\n",
    "        sample_idx += 1\n",
    "\n",
    "print(f\" All {sample_idx} samples saved to {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de975624-c2cb-4bd6-915f-be2e69ff3f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Inception Score evaluation for 20 rounds...\n",
      "Round 1/20 IS: 1.0000\n",
      "Round 2/20 IS: 1.0000\n",
      "Round 3/20 IS: 1.0000\n",
      "Round 4/20 IS: 1.0000\n",
      "Round 5/20 IS: 1.0000\n",
      "Round 6/20 IS: 1.0000\n",
      "Round 7/20 IS: 1.0000\n",
      "Round 8/20 IS: 1.0000\n",
      "Round 9/20 IS: 1.0000\n",
      "Round 10/20 IS: 1.0000\n",
      "Round 11/20 IS: 1.0000\n",
      "Round 12/20 IS: 1.0000\n",
      "Round 13/20 IS: 1.0000\n",
      "Round 14/20 IS: 1.0000\n",
      "Round 15/20 IS: 1.0000\n",
      "Round 16/20 IS: 1.0000\n",
      "Round 17/20 IS: 1.0000\n",
      "Round 18/20 IS: 1.0000\n",
      "Round 19/20 IS: 1.0000\n",
      "Round 20/20 IS: 1.0000\n",
      "\n",
      " Final Inception Score over 20 rounds: 1.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'\n",
    "\n",
    "# Custom Dataset\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.endswith('.png')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Transform\n",
    "transform = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Load InceptionV3\n",
    "inception_model = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "# 20 Rounds\n",
    "inception_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\" Starting Inception Score evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    # Optional: shuffle images each round\n",
    "    all_images = os.listdir(generated_folder)\n",
    "    selected_images = random.sample(all_images, k=1000)\n",
    "\n",
    "    temp_dataset = GeneratedDataset(\n",
    "        folder=generated_folder,\n",
    "        transform=transform\n",
    "    )\n",
    "    temp_dataset.image_paths = [os.path.join(generated_folder, fname) for fname in selected_images]\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in temp_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = inception_model(batch)\n",
    "            preds.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    py = np.mean(preds, axis=0)\n",
    "\n",
    "    scores = []\n",
    "    for pred in preds:\n",
    "        scores.append(np.sum(pred * (np.log(pred + 1e-10) - np.log(py + 1e-10))))\n",
    "\n",
    "    inception_score = np.exp(np.mean(scores))\n",
    "    inception_scores.append(inception_score)\n",
    "\n",
    "    print(f\"Round {round_idx+1}/{rounds} IS: {inception_score:.4f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_is = np.mean(inception_scores)\n",
    "std_is = np.std(inception_scores)\n",
    "print(f\"\\n Final Inception Score over {rounds} rounds: {mean_is:.4f} ± {std_is:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8d7e3c4-6f65-4500-a234-50377d2ba88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting PSNR evaluation for 20 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 1: 100%|██████████| 720/720 [01:07<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1/20 PSNR: 6.68 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 2: 100%|██████████| 720/720 [01:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 2/20 PSNR: 6.73 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 3: 100%|██████████| 720/720 [01:00<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 3/20 PSNR: 6.68 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 4: 100%|██████████| 720/720 [01:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 4/20 PSNR: 6.72 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 5: 100%|██████████| 720/720 [01:00<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 5/20 PSNR: 6.66 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 6: 100%|██████████| 720/720 [01:00<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 6/20 PSNR: 6.68 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 7: 100%|██████████| 720/720 [01:00<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 7/20 PSNR: 6.69 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 8: 100%|██████████| 720/720 [01:00<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 8/20 PSNR: 6.67 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 9: 100%|██████████| 720/720 [01:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 9/20 PSNR: 6.70 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 10: 100%|██████████| 720/720 [01:00<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 10/20 PSNR: 6.71 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 11: 100%|██████████| 720/720 [01:00<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 11/20 PSNR: 6.69 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 12: 100%|██████████| 720/720 [01:00<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 12/20 PSNR: 6.70 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 13: 100%|██████████| 720/720 [01:00<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 13/20 PSNR: 6.71 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 14: 100%|██████████| 720/720 [01:00<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 14/20 PSNR: 6.65 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 15: 100%|██████████| 720/720 [01:00<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 15/20 PSNR: 6.71 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 16: 100%|██████████| 720/720 [01:00<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 16/20 PSNR: 6.66 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 17: 100%|██████████| 720/720 [01:00<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 17/20 PSNR: 6.66 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 18: 100%|██████████| 720/720 [01:00<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 18/20 PSNR: 6.68 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 19: 100%|██████████| 720/720 [01:00<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 19/20 PSNR: 6.70 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 20: 100%|██████████| 720/720 [01:00<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 20/20 PSNR: 6.70 dB\n",
      "\n",
      " final PSNR over 20 rounds: 6.69 ± 0.02 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'   # Your 1000 generated samples\n",
    "real_folder = './DIV2K_train_HR/'             # Real DIV2K images\n",
    "\n",
    "# Transform for real images\n",
    "real_transform = T.Compose([\n",
    "    T.Resize((32, 32)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset for real images\n",
    "class RealDIV2KDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Dataset for generated images\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.endswith('.png')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load real dataset\n",
    "real_dataset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "\n",
    "# 20 rounds\n",
    "psnr_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\" Starting PSNR evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    all_generated = os.listdir(generated_folder)\n",
    "    selected_generated = random.sample(all_generated, k=720)  # 720 to match 90% train\n",
    "\n",
    "    generated_subset = GeneratedDataset(generated_folder, transform=real_transform)\n",
    "    generated_subset.image_paths = [os.path.join(generated_folder, fname) for fname in selected_generated]\n",
    "    generated_loader = DataLoader(generated_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    selected_real = random.sample(real_dataset.image_paths, k=720)\n",
    "    real_subset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "    real_subset.image_paths = selected_real\n",
    "    real_loader = DataLoader(real_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    round_psnr = []\n",
    "\n",
    "    for (gen_img, real_img) in tqdm(zip(generated_loader, real_loader), total=720, desc=f\"Round {round_idx+1}\"):\n",
    "        gen_img = gen_img.squeeze(0)  # remove batch dim\n",
    "        real_img = real_img.squeeze(0)\n",
    "\n",
    "        gen_img_np = gen_img.permute(1, 2, 0).cpu().numpy()\n",
    "        real_img_np = real_img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        psnr_value = compare_psnr(real_img_np, gen_img_np, data_range=1.0)\n",
    "        round_psnr.append(psnr_value)\n",
    "\n",
    "    mean_psnr = np.mean(round_psnr)\n",
    "    psnr_scores.append(mean_psnr)\n",
    "    print(f\" Round {round_idx+1}/{rounds} PSNR: {mean_psnr:.2f} dB\")\n",
    "\n",
    "# Final Report\n",
    "mean_psnr_final = np.mean(psnr_scores)\n",
    "std_psnr_final = np.std(psnr_scores)\n",
    "print(f\"\\n final PSNR over {rounds} rounds: {mean_psnr_final:.2f} ± {std_psnr_final:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d86220a0-aa07-459e-9fc3-345754aea23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting SSIM evaluation for 20 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 1: 100%|██████████| 720/720 [01:01<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1/20 SSIM: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 2: 100%|██████████| 720/720 [01:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 2/20 SSIM: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 3: 100%|██████████| 720/720 [01:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 3/20 SSIM: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 4: 100%|██████████| 720/720 [01:00<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 4/20 SSIM: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 5: 100%|██████████| 720/720 [01:01<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 5/20 SSIM: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 6: 100%|██████████| 720/720 [01:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 6/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 7: 100%|██████████| 720/720 [01:00<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 7/20 SSIM: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 8: 100%|██████████| 720/720 [01:00<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 8/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 9: 100%|██████████| 720/720 [01:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 9/20 SSIM: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 10: 100%|██████████| 720/720 [01:00<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 10/20 SSIM: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 11: 100%|██████████| 720/720 [01:00<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 11/20 SSIM: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 12: 100%|██████████| 720/720 [01:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 12/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 13: 100%|██████████| 720/720 [01:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 13/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 14: 100%|██████████| 720/720 [01:01<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 14/20 SSIM: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 15: 100%|██████████| 720/720 [01:00<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 15/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 16: 100%|██████████| 720/720 [01:00<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 16/20 SSIM: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 17: 100%|██████████| 720/720 [01:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 17/20 SSIM: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 18: 100%|██████████| 720/720 [01:00<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 18/20 SSIM: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 19: 100%|██████████| 720/720 [01:00<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 19/20 SSIM: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 20: 100%|██████████| 720/720 [01:00<00:00, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 20/20 SSIM: 0.0046\n",
      "\n",
      " final SSIM over 20 rounds: 0.0044 ± 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'   # Your 1000 generated samples\n",
    "real_folder = './DIV2K_train_HR/'             # Real DIV2K images\n",
    "\n",
    "# Transform for real images\n",
    "real_transform = T.Compose([\n",
    "    T.Resize((32, 32)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset for real images\n",
    "class RealDIV2KDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Dataset for generated images\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.endswith('.png')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load real dataset\n",
    "real_dataset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "\n",
    "# 20 rounds\n",
    "ssim_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\" Starting SSIM evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    all_generated = os.listdir(generated_folder)\n",
    "    selected_generated = random.sample(all_generated, k=720)  # 720 to match 90% train\n",
    "\n",
    "    generated_subset = GeneratedDataset(generated_folder, transform=real_transform)\n",
    "    generated_subset.image_paths = [os.path.join(generated_folder, fname) for fname in selected_generated]\n",
    "    generated_loader = DataLoader(generated_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    selected_real = random.sample(real_dataset.image_paths, k=720)\n",
    "    real_subset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "    real_subset.image_paths = selected_real\n",
    "    real_loader = DataLoader(real_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    round_ssim = []\n",
    "\n",
    "    for (gen_img, real_img) in tqdm(zip(generated_loader, real_loader), total=720, desc=f\"Round {round_idx+1}\"):\n",
    "        gen_img = gen_img.squeeze(0)  # remove batch dim\n",
    "        real_img = real_img.squeeze(0)\n",
    "\n",
    "        gen_img_np = gen_img.permute(1, 2, 0).cpu().numpy()\n",
    "        real_img_np = real_img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        ssim_value = compare_ssim(real_img_np, gen_img_np, win_size=5, channel_axis=-1, data_range=1.0)\n",
    "        round_ssim.append(ssim_value)\n",
    "\n",
    "    mean_ssim = np.mean(round_ssim)\n",
    "    ssim_scores.append(mean_ssim)\n",
    "    print(f\" Round {round_idx+1}/{rounds} SSIM: {mean_ssim:.4f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_ssim_final = np.mean(ssim_scores)\n",
    "std_ssim_final = np.std(ssim_scores)\n",
    "print(f\"\\n final SSIM over {rounds} rounds: {mean_ssim_final:.4f} ± {std_ssim_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "01b5bc20-3d60-42d5-bc5a-bab693531aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Perceptual Similarity (VGG features) evaluation for 20 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 1: 100%|██████████| 720/720 [01:04<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1/20 Perceptual Similarity: 0.066464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 2: 100%|██████████| 720/720 [01:05<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 2/20 Perceptual Similarity: 0.066026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 3: 100%|██████████| 720/720 [01:04<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 3/20 Perceptual Similarity: 0.066048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 4: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 4/20 Perceptual Similarity: 0.066529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 5: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 5/20 Perceptual Similarity: 0.066572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 6: 100%|██████████| 720/720 [01:04<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 6/20 Perceptual Similarity: 0.065967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 7: 100%|██████████| 720/720 [01:04<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 7/20 Perceptual Similarity: 0.066166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 8: 100%|██████████| 720/720 [01:04<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 8/20 Perceptual Similarity: 0.066192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 9: 100%|██████████| 720/720 [01:04<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 9/20 Perceptual Similarity: 0.066159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 10: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 10/20 Perceptual Similarity: 0.066582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 11: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 11/20 Perceptual Similarity: 0.066148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 12: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 12/20 Perceptual Similarity: 0.066200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 13: 100%|██████████| 720/720 [01:04<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 13/20 Perceptual Similarity: 0.066004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 14: 100%|██████████| 720/720 [01:05<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 14/20 Perceptual Similarity: 0.065730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 15: 100%|██████████| 720/720 [01:04<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 15/20 Perceptual Similarity: 0.066325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 16: 100%|██████████| 720/720 [01:04<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 16/20 Perceptual Similarity: 0.066187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 17: 100%|██████████| 720/720 [01:05<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 17/20 Perceptual Similarity: 0.066063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 18: 100%|██████████| 720/720 [01:04<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 18/20 Perceptual Similarity: 0.065950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 19: 100%|██████████| 720/720 [01:04<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 19/20 Perceptual Similarity: 0.065845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 20: 100%|██████████| 720/720 [01:04<00:00, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 20/20 Perceptual Similarity: 0.065980\n",
      "\n",
      "Final Perceptual Similarity over 20 rounds: 0.066157 ± 0.000231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'   \n",
    "real_folder = './DIV2K_train_HR/'             \n",
    "\n",
    "# Transform for real images\n",
    "real_transform = T.Compose([\n",
    "    T.Resize((224, 224)),  # VGG16 expects 224x224\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset for real images\n",
    "class RealDIV2KDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Dataset for generated images\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(folder, fname) for fname in os.listdir(folder)\n",
    "            if fname.endswith('.png')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Load real dataset\n",
    "real_dataset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "\n",
    "# Load pretrained VGG16 for perceptual feature extraction\n",
    "vgg16 = models.vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "\n",
    "# 20 rounds\n",
    "perceptual_similarity_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\"Starting Perceptual Similarity (VGG features) evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    all_generated = os.listdir(generated_folder)\n",
    "    selected_generated = random.sample(all_generated, k=720)\n",
    "\n",
    "    generated_subset = GeneratedDataset(generated_folder, transform=real_transform)\n",
    "    generated_subset.image_paths = [os.path.join(generated_folder, fname) for fname in selected_generated]\n",
    "    generated_loader = DataLoader(generated_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    selected_real = random.sample(real_dataset.image_paths, k=720)\n",
    "    real_subset = RealDIV2KDataset(real_folder, transform=real_transform)\n",
    "    real_subset.image_paths = selected_real\n",
    "    real_loader = DataLoader(real_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    round_perceptual_similarities = []\n",
    "\n",
    "    for (gen_img, real_img) in tqdm(zip(generated_loader, real_loader), total=720, desc=f\"Round {round_idx+1}\"):\n",
    "        gen_img = gen_img.squeeze(0).unsqueeze(0).to(device)\n",
    "        real_img = real_img.squeeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat_gen = vgg16(gen_img)\n",
    "            feat_real = vgg16(real_img)\n",
    "\n",
    "        # Calculate L2 distance between features\n",
    "        perceptual_distance = F.mse_loss(feat_gen, feat_real, reduction='mean').item()\n",
    "\n",
    "        # Convert to similarity: similarity = 1 / (1 + distance)\n",
    "        perceptual_similarity = 1 / (1 + perceptual_distance)\n",
    "\n",
    "        round_perceptual_similarities.append(perceptual_similarity)\n",
    "\n",
    "    mean_perceptual_similarity = np.mean(round_perceptual_similarities)\n",
    "    perceptual_similarity_scores.append(mean_perceptual_similarity)\n",
    "    print(f\" Round {round_idx+1}/{rounds} Perceptual Similarity: {mean_perceptual_similarity:.6f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_perceptual_similarity_final = np.mean(perceptual_similarity_scores)\n",
    "std_perceptual_similarity_final = np.std(perceptual_similarity_scores)\n",
    "print(f\"\\nFinal Perceptual Similarity over {rounds} rounds: {mean_perceptual_similarity_final:.6f} ± {std_perceptual_similarity_final:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "12a68d7b-2995-4297-8c8e-204d05ee6f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FID evaluation for 20 rounds...\n",
      "Extracting features for Round 1...\n",
      " Round 1/20 FID: 429.05\n",
      "Extracting features for Round 2...\n",
      " Round 2/20 FID: 425.84\n",
      "Extracting features for Round 3...\n",
      " Round 3/20 FID: 425.55\n",
      "Extracting features for Round 4...\n",
      " Round 4/20 FID: 426.97\n",
      "Extracting features for Round 5...\n",
      " Round 5/20 FID: 427.34\n",
      "Extracting features for Round 6...\n",
      " Round 6/20 FID: 424.26\n",
      "Extracting features for Round 7...\n",
      " Round 7/20 FID: 426.08\n",
      "Extracting features for Round 8...\n",
      " Round 8/20 FID: 427.23\n",
      "Extracting features for Round 9...\n",
      " Round 9/20 FID: 426.95\n",
      "Extracting features for Round 10...\n",
      " Round 10/20 FID: 427.29\n",
      "Extracting features for Round 11...\n",
      " Round 11/20 FID: 425.93\n",
      "Extracting features for Round 12...\n",
      " Round 12/20 FID: 425.13\n",
      "Extracting features for Round 13...\n",
      " Round 13/20 FID: 427.46\n",
      "Extracting features for Round 14...\n",
      " Round 14/20 FID: 425.88\n",
      "Extracting features for Round 15...\n",
      " Round 15/20 FID: 424.18\n",
      "Extracting features for Round 16...\n",
      " Round 16/20 FID: 424.08\n",
      "Extracting features for Round 17...\n",
      " Round 17/20 FID: 426.62\n",
      "Extracting features for Round 18...\n",
      " Round 18/20 FID: 425.29\n",
      "Extracting features for Round 19...\n",
      " Round 19/20 FID: 428.04\n",
      "Extracting features for Round 20...\n",
      " Round 20/20 FID: 430.26\n",
      "\n",
      "Final FID over 20 rounds: 426.47 ± 1.55\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "from scipy import linalg\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'\n",
    "real_folder = './DIV2K_train_HR/'\n",
    "\n",
    "# Transform for InceptionV3\n",
    "transform_inception = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset class with directory check\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = []\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            full_path = os.path.join(folder, fname)\n",
    "            if os.path.isfile(full_path) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.image_paths.append(full_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# InceptionV3 model for feature extraction\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.fc = torch.nn.Identity()  # remove final classification layer\n",
    "inception_model.eval()\n",
    "\n",
    "# Extract features from InceptionV3\n",
    "def get_inception_features(dataloader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            preds = inception_model(imgs)\n",
    "            features.append(preds.cpu().numpy())\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features\n",
    "\n",
    "# FID calculation\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# 20 rounds\n",
    "fid_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\"Starting FID evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    # Select 720 random images\n",
    "    selected_gen = random.sample(\n",
    "        [f for f in os.listdir(generated_folder) if os.path.isfile(os.path.join(generated_folder, f))], k=720)\n",
    "    selected_real = random.sample(\n",
    "        [f for f in os.listdir(real_folder) if os.path.isfile(os.path.join(real_folder, f))], k=720)\n",
    "\n",
    "    # Datasets\n",
    "    gen_dataset = CustomImageDataset(generated_folder, transform=transform_inception)\n",
    "    gen_dataset.image_paths = [os.path.join(generated_folder, f) for f in selected_gen]\n",
    "    real_dataset = CustomImageDataset(real_folder, transform=transform_inception)\n",
    "    real_dataset.image_paths = [os.path.join(real_folder, f) for f in selected_real]\n",
    "\n",
    "    # Loaders\n",
    "    gen_loader = DataLoader(gen_dataset, batch_size=batch_size, shuffle=False)\n",
    "    real_loader = DataLoader(real_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Extracting features for Round {round_idx+1}...\")\n",
    "    feat_gen = get_inception_features(gen_loader)\n",
    "    feat_real = get_inception_features(real_loader)\n",
    "\n",
    "    mu_gen, sigma_gen = np.mean(feat_gen, axis=0), np.cov(feat_gen, rowvar=False)\n",
    "    mu_real, sigma_real = np.mean(feat_real, axis=0), np.cov(feat_real, rowvar=False)\n",
    "\n",
    "    fid = calculate_fid(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "    fid_scores.append(fid)\n",
    "\n",
    "    print(f\" Round {round_idx+1}/{rounds} FID: {fid:.2f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_fid = np.mean(fid_scores)\n",
    "std_fid = np.std(fid_scores)\n",
    "print(f\"\\nFinal FID over {rounds} rounds: {mean_fid:.2f} ± {std_fid:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b5ca68-287c-40c9-b481-75f41cee41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KID evaluation for 20 rounds...\n",
      "Extracting features for Round 1...\n",
      " Round 1/20 KID: 0.370075\n",
      "Extracting features for Round 2...\n",
      " Round 2/20 KID: 0.370863\n",
      "Extracting features for Round 3...\n",
      " Round 3/20 KID: 0.371011\n",
      "Extracting features for Round 4...\n",
      " Round 4/20 KID: 0.370363\n",
      "Extracting features for Round 5...\n",
      " Round 5/20 KID: 0.371231\n",
      "Extracting features for Round 6...\n",
      " Round 6/20 KID: 0.370895\n",
      "Extracting features for Round 7...\n",
      " Round 7/20 KID: 0.372200\n",
      "Extracting features for Round 8...\n",
      " Round 8/20 KID: 0.373339\n",
      "Extracting features for Round 9...\n",
      " Round 9/20 KID: 0.371697\n",
      "Extracting features for Round 10...\n",
      " Round 10/20 KID: 0.371470\n",
      "Extracting features for Round 11...\n",
      " Round 11/20 KID: 0.371909\n",
      "Extracting features for Round 12...\n",
      " Round 12/20 KID: 0.371863\n",
      "Extracting features for Round 13...\n",
      " Round 13/20 KID: 0.374367\n",
      "Extracting features for Round 14...\n",
      " Round 14/20 KID: 0.371233\n",
      "Extracting features for Round 15...\n",
      " Round 15/20 KID: 0.372917\n",
      "Extracting features for Round 16...\n",
      " Round 16/20 KID: 0.374129\n",
      "Extracting features for Round 17...\n",
      " Round 17/20 KID: 0.372745\n",
      "Extracting features for Round 18...\n",
      " Round 18/20 KID: 0.372472\n",
      "Extracting features for Round 19...\n",
      " Round 19/20 KID: 0.372971\n",
      "Extracting features for Round 20...\n",
      " Round 20/20 KID: 0.372220\n",
      "\n",
      "Final KID over 20 rounds: 0.371998 ± 0.001141\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "from scipy import linalg\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'   \n",
    "real_folder = './DIV2K_train_HR/'             \n",
    "\n",
    "# Transform for InceptionV3\n",
    "transform_inception = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset class with file filtering\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = []\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            full_path = os.path.join(folder, fname)\n",
    "            if os.path.isfile(full_path) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.image_paths.append(full_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# InceptionV3 model for feature extraction\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.fc = torch.nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "# Extract features\n",
    "def get_inception_features(dataloader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            preds = inception_model(imgs)\n",
    "            features.append(preds.cpu().numpy())\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features\n",
    "\n",
    "# Calculate polynomial kernel (for KID)\n",
    "def polynomial_kernel(X, Y):\n",
    "    return (X @ Y.T / X.shape[1] + 1)**3\n",
    "\n",
    "# Calculate KID\n",
    "def calculate_kid(feat_real, feat_fake):\n",
    "    X = feat_real\n",
    "    Y = feat_fake\n",
    "    K_XX = polynomial_kernel(X, X)\n",
    "    K_YY = polynomial_kernel(Y, Y)\n",
    "    K_XY = polynomial_kernel(X, Y)\n",
    "    m = X.shape[0]\n",
    "    n = Y.shape[0]\n",
    "    sum_K_XX = (np.sum(K_XX) - np.trace(K_XX)) / (m * (m - 1))\n",
    "    sum_K_YY = (np.sum(K_YY) - np.trace(K_YY)) / (n * (n - 1))\n",
    "    sum_K_XY = np.sum(K_XY) / (m * n)\n",
    "    return sum_K_XX + sum_K_YY - 2 * sum_K_XY\n",
    "\n",
    "# 20 rounds\n",
    "kid_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\"Starting KID evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    selected_gen = random.sample(\n",
    "        [f for f in os.listdir(generated_folder) if os.path.isfile(os.path.join(generated_folder, f))], k=720)\n",
    "    selected_real = random.sample(\n",
    "        [f for f in os.listdir(real_folder) if os.path.isfile(os.path.join(real_folder, f))], k=720)\n",
    "\n",
    "    gen_dataset = CustomImageDataset(generated_folder, transform=transform_inception)\n",
    "    gen_dataset.image_paths = [os.path.join(generated_folder, f) for f in selected_gen]\n",
    "    real_dataset = CustomImageDataset(real_folder, transform=transform_inception)\n",
    "    real_dataset.image_paths = [os.path.join(real_folder, f) for f in selected_real]\n",
    "\n",
    "    gen_loader = DataLoader(gen_dataset, batch_size=batch_size, shuffle=False)\n",
    "    real_loader = DataLoader(real_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Extracting features for Round {round_idx+1}...\")\n",
    "    feat_gen = get_inception_features(gen_loader)\n",
    "    feat_real = get_inception_features(real_loader)\n",
    "\n",
    "    kid = calculate_kid(feat_real, feat_gen)\n",
    "    kid_scores.append(kid)\n",
    "\n",
    "    print(f\" Round {round_idx+1}/{rounds} KID: {kid:.6f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_kid = np.mean(kid_scores)\n",
    "std_kid = np.std(kid_scores)\n",
    "print(f\"\\nFinal KID over {rounds} rounds: {mean_kid:.6f} ± {std_kid:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e9c2af-bd15-4b86-be25-566cb030d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Starting LPIPS evaluation for 20 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 1: 100%|██████████| 720/720 [01:18<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1/20 LPIPS: 0.818397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 2: 100%|██████████| 720/720 [01:11<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 2/20 LPIPS: 0.817274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 3: 100%|██████████| 720/720 [01:09<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 3/20 LPIPS: 0.816603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 4: 100%|██████████| 720/720 [01:08<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 4/20 LPIPS: 0.818155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 5: 100%|██████████| 720/720 [01:08<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 5/20 LPIPS: 0.816586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 6: 100%|██████████| 720/720 [01:13<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 6/20 LPIPS: 0.817939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 7: 100%|██████████| 720/720 [01:11<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 7/20 LPIPS: 0.818558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 8: 100%|██████████| 720/720 [01:09<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 8/20 LPIPS: 0.818252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 9: 100%|██████████| 720/720 [01:08<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 9/20 LPIPS: 0.816322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 10: 100%|██████████| 720/720 [01:08<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 10/20 LPIPS: 0.818220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 11: 100%|██████████| 720/720 [01:08<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 11/20 LPIPS: 0.817342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 12: 100%|██████████| 720/720 [01:09<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 12/20 LPIPS: 0.817518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 13: 100%|██████████| 720/720 [01:12<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 13/20 LPIPS: 0.817283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 14: 100%|██████████| 720/720 [01:10<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 14/20 LPIPS: 0.817148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 15: 100%|██████████| 720/720 [01:08<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 15/20 LPIPS: 0.817855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 16: 100%|██████████| 720/720 [01:08<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 16/20 LPIPS: 0.815939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 17: 100%|██████████| 720/720 [01:09<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 17/20 LPIPS: 0.818788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 18: 100%|██████████| 720/720 [01:08<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 18/20 LPIPS: 0.817309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 19: 100%|██████████| 720/720 [01:08<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 19/20 LPIPS: 0.817253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round 20: 100%|██████████| 720/720 [01:08<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 20/20 LPIPS: 0.817783\n",
      "\n",
      "Final LPIPS over 20 rounds: 0.817526 ± 0.000751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import lpips\n",
    "\n",
    "# Settings\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generated_folder = './generated_div2k_32/'\n",
    "real_folder = './DIV2K_train_HR/'\n",
    "\n",
    "# Transform for LPIPS model\n",
    "transform_lpips = T.Compose([\n",
    "    T.Resize((256, 256)),  # LPIPS can work with 256x256 size easily\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset class with directory check\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = []\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            full_path = os.path.join(folder, fname)\n",
    "            if os.path.isfile(full_path) and fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.image_paths.append(full_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device)  # You can change 'alex' to 'vgg' or 'squeeze' if needed\n",
    "lpips_model.eval()\n",
    "\n",
    "# 20 rounds\n",
    "lpips_scores = []\n",
    "rounds = 20\n",
    "\n",
    "print(f\"Starting LPIPS evaluation for {rounds} rounds...\")\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    selected_gen = random.sample(\n",
    "        [f for f in os.listdir(generated_folder) if os.path.isfile(os.path.join(generated_folder, f))], k=720)\n",
    "    selected_real = random.sample(\n",
    "        [f for f in os.listdir(real_folder) if os.path.isfile(os.path.join(real_folder, f))], k=720)\n",
    "\n",
    "    gen_dataset = CustomImageDataset(generated_folder, transform=transform_lpips)\n",
    "    gen_dataset.image_paths = [os.path.join(generated_folder, f) for f in selected_gen]\n",
    "    real_dataset = CustomImageDataset(real_folder, transform=transform_lpips)\n",
    "    real_dataset.image_paths = [os.path.join(real_folder, f) for f in selected_real]\n",
    "\n",
    "    gen_loader = DataLoader(gen_dataset, batch_size=1, shuffle=False)\n",
    "    real_loader = DataLoader(real_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    round_lpips = []\n",
    "\n",
    "    for (gen_img, real_img) in tqdm(zip(gen_loader, real_loader), total=720, desc=f\"Round {round_idx+1}\"):\n",
    "        gen_img = gen_img.squeeze(0).unsqueeze(0).to(device)  # Shape (1, 3, 256, 256)\n",
    "        real_img = real_img.squeeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lpips_value = lpips_model(gen_img, real_img).item()\n",
    "\n",
    "        round_lpips.append(lpips_value)\n",
    "\n",
    "    mean_lpips = np.mean(round_lpips)\n",
    "    lpips_scores.append(mean_lpips)\n",
    "\n",
    "    print(f\" Round {round_idx+1}/{rounds} LPIPS: {mean_lpips:.6f}\")\n",
    "\n",
    "# Final Report\n",
    "mean_lpips_final = np.mean(lpips_scores)\n",
    "std_lpips_final = np.std(lpips_scores)\n",
    "print(f\"\\nFinal LPIPS over {rounds} rounds: {mean_lpips_final:.6f} ± {std_lpips_final:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f462c-3069-4382-a8eb-64d4649ec23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /users/PFS0270/nikhilchatta/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!git add .\n",
    "!git commit -m \"LDM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651e00b-7b3b-4409-8032-bc87f1178e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "git remote add origin git@github.com:nikhilchatta/LatentDiffusion.git\n",
    "git branch -M main\n",
    "git push -u origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a07b3-9171-4329-989e-0de0c6c731c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
